{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2425d3-1539-42a9-8b64-b9228ae67a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True, dropout=0.3):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=bias\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        combined_conv = self.dropout(combined_conv)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=True, dropout=0.3):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        kernel_size = [kernel_size] * num_layers if isinstance(kernel_size, int) else kernel_size\n",
    "        hidden_dim = [hidden_dim] * num_layers if isinstance(hidden_dim, int) else hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.cell_list = nn.ModuleList([\n",
    "            ConvLSTMCell(\n",
    "                input_dim=self.input_dim if i == 0 else self.hidden_dim[i-1],\n",
    "                hidden_dim=self.hidden_dim[i],\n",
    "                kernel_size=self.kernel_size[i],\n",
    "                bias=True,\n",
    "                dropout=dropout if i < num_layers-1 else 0\n",
    "            ) for i in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "        b, t, c, h, w = input_tensor.size()\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        cur_layer_input = input_tensor\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t_idx in range(t):\n",
    "                h, c = self.cell_list[layer_idx](cur_layer_input[:, t_idx, :, :, :], cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "        return layer_output_list[-1], last_state_list[-1]\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        return [self.cell_list[i].init_hidden(batch_size, image_size) for i in range(self.num_layers)]\n",
    "\n",
    "\n",
    "class HeadCNN(nn.Module):\n",
    "    def __init__(self, input_dim=16, hidden_dim=32, dropout=0.3):\n",
    "        super(HeadCNN, self).__init__()\n",
    "        self.convlstm = ConvLSTM(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=3, num_layers=1, dropout=dropout)\n",
    "        self.out_conv = nn.Conv2d(hidden_dim, 1, kernel_size=1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_output, _ = self.convlstm(x)\n",
    "        b, t, c, h, w = layer_output.size()\n",
    "        layer_output = layer_output.view(b * t, c, h, w)\n",
    "        out = self.out_conv(layer_output)\n",
    "        out = self.activation(out)\n",
    "        out = out.view(b, t, 1, h, w)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConcCNN(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=32, dropout=0.3):\n",
    "        super(ConcCNN, self).__init__()\n",
    "        self.convlstm = ConvLSTM(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=3, num_layers=1, dropout=dropout)\n",
    "        self.out_conv = nn.Conv2d(hidden_dim, 1, kernel_size=1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_output, _ = self.convlstm(x)\n",
    "        b, t, c, h, w = layer_output.size()\n",
    "        layer_output = layer_output.view(b * t, c, h, w)\n",
    "        out = self.out_conv(layer_output)\n",
    "        out = self.activation(out)\n",
    "        out = out.view(b, t, 1, h, w)\n",
    "        return out\n",
    "\n",
    "# Dataset Class - ä¿®å¤ç´¢å¼•é—®é¢˜\n",
    "class HydroCNNDataset(Dataset):\n",
    "    def __init__(self, data, grid_size, max_time_steps):\n",
    "       \n",
    "        self.data = data.reset_index(drop=True).copy()\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.models = self.data['model_name'].unique()\n",
    "        self.cached_data = {}\n",
    "        \n",
    "       \n",
    "        self.base_feature_cols = [\n",
    "            'x', 'y', 'top', 'bottom', 'K', 'recharge', 'ET',\n",
    "            'river_stage', 'river_cond', 'river_rbot', 'well_rate', 'well_mask',\n",
    "            'chd_mask', 'lytyp'\n",
    "        ]\n",
    "        \n",
    "        self.conc_base_feature_cols = self.base_feature_cols + ['conc_mask']\n",
    "        \n",
    "        self._normalize_features()\n",
    "        self._preprocess_data()\n",
    "\n",
    "    def _normalize_features(self):\n",
    "        \"\"\"æ ‡å‡†åŒ–ç‰¹å¾ï¼Œä¸GNNä¿æŒä¸€è‡´\"\"\"\n",
    "        print(\"å¼€å§‹ç‰¹å¾æ ‡å‡†åŒ–...\")\n",
    "        \n",
    "       \n",
    "        float_cols = [col for col in self.base_feature_cols if col not in ['well_mask', 'chd_mask', 'lytyp']]\n",
    "        \n",
    "        for model_name in self.models:\n",
    "            model_df = self.data[self.data['model_name'] == model_name].copy()\n",
    "            \n",
    "            # æ ‡å‡†åŒ–åŸºç¡€æµ®ç‚¹ç‰¹å¾\n",
    "            if len(model_df) > 0:  \n",
    "                scaler = StandardScaler()\n",
    "                float_data = model_df[float_cols].values\n",
    "                if float_data.size > 0: \n",
    "                    scaled_data = scaler.fit_transform(float_data)\n",
    "                  \n",
    "                    mask = self.data['model_name'] == model_name\n",
    "                    self.data.loc[mask, float_cols] = scaled_data\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "  \n",
    "        M, N = self.grid_size\n",
    "        T = self.max_time_steps\n",
    "        \n",
    "        for model_idx, model_name in enumerate(self.models):\n",
    "            print(f\"å¤„ç†æ¨¡å‹ {model_idx+1}/{len(self.models)}: {model_name}\")\n",
    "            \n",
    "            \n",
    "            model_df = self.data[self.data['model_name'] == model_name].copy().reset_index(drop=True)\n",
    "            \n",
    "            if len(model_df) == 0:\n",
    "                print(f\"è­¦å‘Š: æ¨¡å‹ {model_name} æ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            model_df['time_step'] = model_df['time_step'] - model_df['time_step'].min()\n",
    "            \n",
    "            \n",
    "            model_df = model_df.sort_values(['row', 'col', 'time_step']).reset_index(drop=True)\n",
    "            \n",
    "            print(f\"  è®¡ç®—å†å²ç‰¹å¾...\")\n",
    "          \n",
    "            model_df['prev_head'] = 0.0\n",
    "            model_df['prev2_head'] = 0.0\n",
    "            model_df['prev_conc'] = 0.0\n",
    "            model_df['prev2_conc'] = 0.0\n",
    "            \n",
    "           \n",
    "            for (row, col), group in model_df.groupby(['row', 'col']):\n",
    "                group_sorted = group.sort_values('time_step')\n",
    "                indices = group_sorted.index\n",
    "                \n",
    "                if len(indices) > 0:\n",
    "                    head_vals = group_sorted['head'].values\n",
    "                    conc_vals = group_sorted['concentration'].values\n",
    "                    \n",
    "                   \n",
    "                    prev_head_vals = np.concatenate([[head_vals[0]], head_vals[:-1]])\n",
    "                    prev_conc_vals = np.concatenate([[conc_vals[0]], conc_vals[:-1]])\n",
    "                    \n",
    "                    \n",
    "                    if len(head_vals) >= 2:\n",
    "                        prev2_head_vals = np.concatenate([[head_vals[0]], [head_vals[0]], head_vals[:-2]])\n",
    "                        prev2_conc_vals = np.concatenate([[conc_vals[0]], [conc_vals[0]], conc_vals[:-2]])\n",
    "                    else:\n",
    "                        prev2_head_vals = np.full(len(head_vals), head_vals[0])\n",
    "                        prev2_conc_vals = np.full(len(conc_vals), conc_vals[0])\n",
    "                    \n",
    "                    model_df.loc[indices, 'prev_head'] = prev_head_vals\n",
    "                    model_df.loc[indices, 'prev2_head'] = prev2_head_vals\n",
    "                    model_df.loc[indices, 'prev_conc'] = prev_conc_vals\n",
    "                    model_df.loc[indices, 'prev2_conc'] = prev2_conc_vals\n",
    "            \n",
    "          \n",
    "            head_feature_cols = self.base_feature_cols + ['prev_head', 'prev2_head']\n",
    "            conc_feature_cols = self.conc_base_feature_cols + ['prev_head', 'prev2_head', 'prev_conc', 'prev2_conc']\n",
    "            \n",
    "       \n",
    "            X_head = np.zeros((T, len(head_feature_cols), M, N), dtype=np.float32)\n",
    "            X_conc_base = np.zeros((T, len(conc_feature_cols) - 1, M, N), dtype=np.float32)\n",
    "            Y_head = np.zeros((T, 1, M, N), dtype=np.float32)\n",
    "            Y_conc = np.zeros((T, 1, M, N), dtype=np.float32)\n",
    "            mask = np.zeros((M, N), dtype=np.float32)\n",
    "            \n",
    "            print(f\"  å¡«å……ç½‘æ ¼æ•°æ®...\")\n",
    "            \n",
    "            \n",
    "            max_t = min(T, model_df['time_step'].max() + 1)\n",
    "            for t in range(max_t):\n",
    "                if t % 50 == 0: \n",
    "                    print(f\"    å¤„ç†æ—¶é—´æ­¥ {t}/{max_t}\")\n",
    "                    \n",
    "                t_df = model_df[model_df['time_step'] == t]\n",
    "                if len(t_df) == 0:\n",
    "                    continue\n",
    "                \n",
    "                rows = t_df['row'].values.astype(int)\n",
    "                cols = t_df['col'].values.astype(int)\n",
    "                \n",
    "               \n",
    "                valid_mask = (rows >= 0) & (rows < M) & (cols >= 0) & (cols < N)\n",
    "                if not np.any(valid_mask):\n",
    "                    continue\n",
    "                    \n",
    "                rows = rows[valid_mask]\n",
    "                cols = cols[valid_mask]\n",
    "                t_df_valid = t_df.iloc[valid_mask]\n",
    "                \n",
    "               \n",
    "                head_data = t_df_valid[head_feature_cols].values  # Shape: (n_points, 16)\n",
    "                for feat_idx in range(len(head_feature_cols)):\n",
    "                    X_head[t, feat_idx, rows, cols] = head_data[:, feat_idx]\n",
    "                \n",
    "               \n",
    "                conc_data = t_df_valid[conc_feature_cols[:-1]].values  # Shape: (n_points, 18)\n",
    "                for feat_idx in range(len(conc_feature_cols) - 1):\n",
    "                    X_conc_base[t, feat_idx, rows, cols] = conc_data[:, feat_idx]\n",
    "                \n",
    "                \n",
    "                Y_head[t, 0, rows, cols] = t_df_valid['head'].values\n",
    "                Y_conc[t, 0, rows, cols] = t_df_valid['concentration'].values\n",
    "                \n",
    "              \n",
    "                mask[rows, cols] = 1\n",
    "            \n",
    "            self.cached_data[model_name] = {\n",
    "                'X_head': torch.from_numpy(X_head),\n",
    "                'X_conc_base': torch.from_numpy(X_conc_base),\n",
    "                'Y_head': torch.from_numpy(Y_head),\n",
    "                'Y_conc': torch.from_numpy(Y_conc),\n",
    "                'mask': torch.from_numpy(mask),\n",
    "                'model_name': model_name\n",
    "            }\n",
    "            \n",
    "            print(f\"  æ¨¡å‹ {model_name} å¤„ç†å®Œæˆ\")\n",
    "        \n",
    "        print(f\"é¢„å¤„ç†å®Œæˆï¼å¤„ç†äº† {len(self.cached_data)} ä¸ªæ¨¡å‹\")\n",
    "        if len(self.cached_data) > 0:\n",
    "            sample_data = list(self.cached_data.values())[0]\n",
    "            print(f\"ç‰¹å¾ç»´åº¦ - æ°´å¤´: {sample_data['X_head'].shape[1]}, æµ“åº¦åŸºç¡€: {sample_data['X_conc_base'].shape[1]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cached_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        model_name = list(self.cached_data.keys())[idx]\n",
    "        return self.cached_data[model_name]\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    fixed_keys = ['X_head', 'X_conc_base', 'Y_head', 'Y_conc', 'mask']\n",
    "    variable_keys = ['model_name']\n",
    "    collated = {}\n",
    "    for key in fixed_keys:\n",
    "        collated[key] = torch.stack([item[key] for item in batch])\n",
    "    for key in variable_keys:\n",
    "        collated[key] = [item[key] for item in batch]\n",
    "    return collated\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, mask, T):\n",
    "    \"\"\"è®¡ç®—æŒ‡æ ‡ï¼Œä¸GNNä¿æŒä¸€è‡´\"\"\"\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.detach().cpu().numpy()\n",
    "\n",
    " \n",
    "    mask = mask[:, np.newaxis, np.newaxis, :, :]  # Shape: [B, 1, 1, M, N]\n",
    "    mask = np.repeat(mask, T, axis=1)  # Shape: [B, T, 1, M, N]\n",
    "\n",
    "    y_true = y_true[mask > 0]\n",
    "    y_pred = y_pred[mask > 0]\n",
    "\n",
    "    valid_mask = ~np.isnan(y_true) & ~np.isinf(y_true) & ~np.isnan(y_pred) & ~np.isinf(y_pred)\n",
    "    y_true = y_true[valid_mask]\n",
    "    y_pred = y_pred[valid_mask]\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        return {'mse': np.nan, 'rmse': np.nan, 'mae': np.nan, 'r2': np.nan}\n",
    "\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # è®¡ç®—R2\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "\n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "\n",
    "def train_dual_cnn(train_loader, val_loader, config):\n",
    "    \"\"\"åˆ†æ­¥è®­ç»ƒï¼šå…ˆè®­ç»ƒæ°´å¤´æ¨¡å‹ï¼Œå†è®­ç»ƒæµ“åº¦æ¨¡å‹\"\"\"\n",
    "    \n",
    "   \n",
    "    os.makedirs(config['save_path'], exist_ok=True)\n",
    "    \n",
    "    print(\"ğŸ”¹ å¼€å§‹è®­ç»ƒæ°´å¤´æ¨¡å‹...\")\n",
    "    \n",
    "\n",
    "    head_model = HeadCNN(input_dim=16, hidden_dim=config['hidden_dim']).to(device)\n",
    "    head_optimizer = torch.optim.AdamW(\n",
    "        head_model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    head_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(head_optimizer, T_max=config['num_epochs'])\n",
    "    \n",
    "    best_head_val_loss = float('inf')\n",
    "    head_early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "\n",
    "        head_model.train()\n",
    "        total_head_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            X_head = batch['X_head'].to(device)\n",
    "            Y_head = batch['Y_head'].to(device)\n",
    "            mask = batch['mask'].to(device).unsqueeze(1).unsqueeze(1)  # [B, 1, 1, M, N]\n",
    "            \n",
    "            head_optimizer.zero_grad()\n",
    "            pred_head = head_model(X_head)\n",
    "            \n",
    "           \n",
    "            loss_head = F.mse_loss(pred_head * mask, Y_head * mask)\n",
    "            loss_head.backward()\n",
    "            \n",
    "           \n",
    "            torch.nn.utils.clip_grad_norm_(head_model.parameters(), max_norm=1.0)\n",
    "            head_optimizer.step()\n",
    "            \n",
    "            total_head_loss += loss_head.item()\n",
    "        \n",
    "     \n",
    "        head_model.eval()\n",
    "        total_head_val_loss = 0\n",
    "        all_head_metrics = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                X_head = batch['X_head'].to(device)\n",
    "                Y_head = batch['Y_head'].to(device)\n",
    "                mask = batch['mask'].to(device).unsqueeze(1).unsqueeze(1)\n",
    "                \n",
    "                pred_head = head_model(X_head)\n",
    "                loss_head = F.mse_loss(pred_head * mask, Y_head * mask)\n",
    "                total_head_val_loss += loss_head.item()\n",
    "                \n",
    "              \n",
    "                head_metrics = compute_metrics(Y_head, pred_head, batch['mask'], config['max_time_steps'])\n",
    "                all_head_metrics.append(head_metrics)\n",
    "        \n",
    "        avg_head_train_loss = total_head_loss / len(train_loader)\n",
    "        avg_head_val_loss = total_head_val_loss / len(val_loader)\n",
    "        \n",
    "     \n",
    "        avg_head_r2 = np.nanmean([m['r2'] for m in all_head_metrics])\n",
    "        avg_head_rmse = np.nanmean([m['rmse'] for m in all_head_metrics])\n",
    "        \n",
    "        head_scheduler.step()\n",
    "        current_lr = head_scheduler.get_last_lr()[0]\n",
    "        \n",
    "        print(f\"æ°´å¤´æ¨¡å‹ Epoch {epoch+1:03d}/{config['num_epochs']} | \"\n",
    "              f\"è®­ç»ƒæŸå¤±: {avg_head_train_loss:.4f} | éªŒè¯æŸå¤±: {avg_head_val_loss:.4f} | \"\n",
    "              f\"R2: {avg_head_r2:.4f} | RMSE: {avg_head_rmse:.4f} | LR: {current_lr:.6f}\")\n",
    "        \n",
    "      \n",
    "        if avg_head_val_loss < best_head_val_loss:\n",
    "            best_head_val_loss = avg_head_val_loss\n",
    "            head_early_stop_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': head_model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'train_loss': avg_head_train_loss,\n",
    "                'val_loss': avg_head_val_loss,\n",
    "                'r2': avg_head_r2,\n",
    "                'config': config\n",
    "            }, os.path.join(config['save_path'], 'best_head_model.pth'))\n",
    "            print(f\"ä¿å­˜æœ€ä½³æ°´å¤´æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_head_val_loss:.4f}\")\n",
    "        else:\n",
    "            head_early_stop_counter += 1\n",
    "        \n",
    "        # æ—©åœæ£€æŸ¥\n",
    "        if head_early_stop_counter >= config['patience']:\n",
    "            print(f\"æ°´å¤´æ¨¡å‹æ—©åœè§¦å‘! åœ¨ç¬¬{epoch+1}ä¸ªepochåœæ­¢è®­ç»ƒ\")\n",
    "            break\n",
    "    \n",
    "   \n",
    "    best_head_checkpoint = torch.load(os.path.join(config['save_path'], 'best_head_model.pth'))\n",
    "    head_model.load_state_dict(best_head_checkpoint['model_state_dict'])\n",
    "    head_model.eval()\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ æ°´å¤´æ¨¡å‹è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_head_val_loss:.4f}\")\n",
    "    print(\"ğŸ”¹ å¼€å§‹è®­ç»ƒæµ“åº¦æ¨¡å‹...\")\n",
    "    \n",
    "\n",
    "    conc_model = ConcCNN(input_dim=20, hidden_dim=config['hidden_dim']).to(device)\n",
    "    conc_optimizer = torch.optim.AdamW(\n",
    "        conc_model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    conc_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(conc_optimizer, T_max=config['num_epochs'])\n",
    "    \n",
    "    best_conc_val_loss = float('inf')\n",
    "    best_conc_r2 = float('-inf')\n",
    "    conc_early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "       \n",
    "        conc_model.train()\n",
    "        total_conc_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            X_head = batch['X_head'].to(device)\n",
    "            X_conc_base = batch['X_conc_base'].to(device)\n",
    "            Y_conc = batch['Y_conc'].to(device)\n",
    "            mask = batch['mask'].to(device).unsqueeze(1).unsqueeze(1)\n",
    "            \n",
    "       \n",
    "            with torch.no_grad():\n",
    "                pred_head = head_model(X_head)\n",
    "            \n",
    "           \n",
    "            X_conc = torch.cat([X_conc_base, pred_head], dim=2)  # [B, T, 19, M, N]\n",
    "            \n",
    "            conc_optimizer.zero_grad()\n",
    "            pred_conc = conc_model(X_conc)\n",
    "            \n",
    "           \n",
    "            loss_conc = F.mse_loss(pred_conc * mask, Y_conc * mask)\n",
    "            loss_conc.backward()\n",
    "            \n",
    "         \n",
    "            torch.nn.utils.clip_grad_norm_(conc_model.parameters(), max_norm=1.0)\n",
    "            conc_optimizer.step()\n",
    "            \n",
    "            total_conc_loss += loss_conc.item()\n",
    "        \n",
    "    \n",
    "        conc_model.eval()\n",
    "        total_conc_val_loss = 0\n",
    "        all_conc_metrics = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                X_head = batch['X_head'].to(device)\n",
    "                X_conc_base = batch['X_conc_base'].to(device)\n",
    "                Y_conc = batch['Y_conc'].to(device)\n",
    "                mask = batch['mask'].to(device).unsqueeze(1).unsqueeze(1)\n",
    "                \n",
    "                pred_head = head_model(X_head)\n",
    "                X_conc = torch.cat([X_conc_base, pred_head], dim=2)\n",
    "                pred_conc = conc_model(X_conc)\n",
    "                \n",
    "                loss_conc = F.mse_loss(pred_conc * mask, Y_conc * mask)\n",
    "                total_conc_val_loss += loss_conc.item()\n",
    "                \n",
    "              \n",
    "                conc_metrics = compute_metrics(Y_conc, pred_conc, batch['mask'], config['max_time_steps'])\n",
    "                all_conc_metrics.append(conc_metrics)\n",
    "        \n",
    "        avg_conc_train_loss = total_conc_loss / len(train_loader)\n",
    "        avg_conc_val_loss = total_conc_val_loss / len(val_loader)\n",
    "        \n",
    "  \n",
    "        avg_conc_r2 = np.nanmean([m['r2'] for m in all_conc_metrics])\n",
    "        avg_conc_rmse = np.nanmean([m['rmse'] for m in all_conc_metrics])\n",
    "        \n",
    "        conc_scheduler.step()\n",
    "        current_lr = conc_scheduler.get_last_lr()[0]\n",
    "        \n",
    "        print(f\"æµ“åº¦æ¨¡å‹ Epoch {epoch+1:03d}/{config['num_epochs']} | \"\n",
    "              f\"è®­ç»ƒæŸå¤±: {avg_conc_train_loss:.4f} | éªŒè¯æŸå¤±: {avg_conc_val_loss:.4f} | \"\n",
    "              f\"R2: {avg_conc_r2:.4f} | RMSE: {avg_conc_rmse:.4f} | LR: {current_lr:.6f}\")\n",
    "        \n",
    "       \n",
    "        if avg_conc_val_loss < best_conc_val_loss:\n",
    "            best_conc_val_loss = avg_conc_val_loss\n",
    "            torch.save({\n",
    "                'head_model_state_dict': head_model.state_dict(),\n",
    "                'conc_model_state_dict': conc_model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'train_loss': avg_conc_train_loss,\n",
    "                'val_loss': avg_conc_val_loss,\n",
    "                'r2': avg_conc_r2,\n",
    "                'config': config,\n",
    "                'criterion': 'loss'\n",
    "            }, os.path.join(config['save_path'], 'best_conc_model_loss.pth'))\n",
    "        \n",
    "\n",
    "        if avg_conc_r2 > best_conc_r2:\n",
    "            best_conc_r2 = avg_conc_r2\n",
    "            conc_early_stop_counter = 0\n",
    "            torch.save({\n",
    "                'head_model_state_dict': head_model.state_dict(),\n",
    "                'conc_model_state_dict': conc_model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'train_loss': avg_conc_train_loss,\n",
    "                'val_loss': avg_conc_val_loss,\n",
    "                'r2': avg_conc_r2,\n",
    "                'config': config,\n",
    "                'criterion': 'r2'\n",
    "            }, os.path.join(config['save_path'], 'best_conc_model_r2.pth'))\n",
    "            print(f\"ä¿å­˜åŸºäºR2çš„æœ€ä½³æµ“åº¦æ¨¡å‹ï¼ŒR2: {best_conc_r2:.4f}\")\n",
    "        else:\n",
    "            conc_early_stop_counter += 1\n",
    "        \n",
    "        # æ—©åœæ£€æŸ¥\n",
    "        if conc_early_stop_counter >= config['patience']:\n",
    "            print(f\"æµ“åº¦æ¨¡å‹æ—©åœè§¦å‘! åœ¨ç¬¬{epoch+1}ä¸ªepochåœæ­¢è®­ç»ƒ\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ æµ“åº¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n",
    "    print(f\"æœ€ä½³éªŒè¯æŸå¤±: {best_conc_val_loss:.4f}\")\n",
    "    print(f\"æœ€ä½³R2: {best_conc_r2:.4f}\")\n",
    "    \n",
    "    return head_model, conc_model\n",
    "\n",
    "\n",
    "def evaluate_dual_cnn(data_loader, config):\n",
    "\n",
    "    checkpoint = torch.load(os.path.join(config['save_path'], 'best_conc_model_r2.pth'))\n",
    "    \n",
    "    head_model = HeadCNN(input_dim=16, hidden_dim=config['hidden_dim']).to(device)\n",
    "    conc_model = ConcCNN(input_dim=19, hidden_dim=config['hidden_dim']).to(device)\n",
    "    \n",
    "    head_model.load_state_dict(checkpoint['head_model_state_dict'])\n",
    "    conc_model.load_state_dict(checkpoint['conc_model_state_dict'])\n",
    "    \n",
    "    head_model.eval()\n",
    "    conc_model.eval()\n",
    "    \n",
    "    all_head_metrics = []\n",
    "    all_conc_metrics = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    print(\"å¼€å§‹æ¨¡å‹è¯„ä¼°...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            X_head = batch['X_head'].to(device)\n",
    "            X_conc_base = batch['X_conc_base'].to(device)\n",
    "            Y_head = batch['Y_head'].to(device)\n",
    "            Y_conc = batch['Y_conc'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            model_names = batch['model_name']\n",
    "            \n",
    "    \n",
    "            pred_head = head_model(X_head)\n",
    "            \n",
    "         \n",
    "            X_conc = torch.cat([X_conc_base, pred_head], dim=2)\n",
    "            pred_conc = conc_model(X_conc)\n",
    "            \n",
    "         \n",
    "            for i in range(len(model_names)):\n",
    "                head_metrics = compute_metrics(Y_head[i:i+1], pred_head[i:i+1], mask[i:i+1], config['max_time_steps'])\n",
    "                conc_metrics = compute_metrics(Y_conc[i:i+1], pred_conc[i:i+1], mask[i:i+1], config['max_time_steps'])\n",
    "                \n",
    "                all_head_metrics.append(head_metrics)\n",
    "                all_conc_metrics.append(conc_metrics)\n",
    "                \n",
    "              \n",
    "                all_predictions.append({\n",
    "                    'model_name': model_names[i],\n",
    "                    'head_r2': head_metrics['r2'],\n",
    "                    'head_rmse': head_metrics['rmse'],\n",
    "                    'conc_r2': conc_metrics['r2'],\n",
    "                    'conc_rmse': conc_metrics['rmse']\n",
    "                })\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡æŒ‡æ ‡\n",
    "    avg_head_metrics = {\n",
    "        'mse': np.nanmean([m['mse'] for m in all_head_metrics]),\n",
    "        'rmse': np.nanmean([m['rmse'] for m in all_head_metrics]),\n",
    "        'mae': np.nanmean([m['mae'] for m in all_head_metrics]),\n",
    "        'r2': np.nanmean([m['r2'] for m in all_head_metrics])\n",
    "    }\n",
    "    \n",
    "    avg_conc_metrics = {\n",
    "        'mse': np.nanmean([m['mse'] for m in all_conc_metrics]),\n",
    "        'rmse': np.nanmean([m['rmse'] for m in all_conc_metrics]),\n",
    "        'mae': np.nanmean([m['mae'] for m in all_conc_metrics]),\n",
    "        'r2': np.nanmean([m['r2'] for m in all_conc_metrics])\n",
    "    }\n",
    "    \n",
    "    # è¾“å‡ºç»“æœ\n",
    "    print(\"\\nğŸ“Š CNNæ¨¡å‹éªŒè¯ç»“æœ:\")\n",
    "    print(\"\\nğŸ”¹ æ°´å¤´æŒ‡æ ‡:\")\n",
    "    for k, v in avg_head_metrics.items():\n",
    "        print(f\"{k.upper():<5}: {v:.4f}\")\n",
    "    print(\"\\nğŸ”¹ æµ“åº¦æŒ‡æ ‡:\")\n",
    "    for k, v in avg_conc_metrics.items():\n",
    "        print(f\"{k.upper():<5}: {v:.4f}\")\n",
    "    \n",
    "\n",
    "    predictions_df = pd.DataFrame(all_predictions)\n",
    "    predictions_df.to_csv(os.path.join(config['save_path'], 'val_predictions_cnn.csv'), index=False)\n",
    "    print(f\"\\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {os.path.join(config['save_path'], 'val_predictions_cnn.csv')}\")\n",
    "    \n",
    "    return avg_head_metrics, avg_conc_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cleaned_data = pd.read_csv('conc_dual_guass.csv')\n",
    "    print(\"æ•°æ®ç»Ÿè®¡:\")\n",
    "    print(cleaned_data[['head', 'concentration']].describe())\n",
    "    \n",
    "\n",
    "    required_cols = [\n",
    "        'row', 'col', 'time_step', 'x', 'y', 'top', 'bottom', 'K', 'recharge', 'ET',\n",
    "        'river_stage', 'river_cond', 'river_rbot', 'well_rate', 'well_mask',\n",
    "        'chd_mask', 'lytyp', 'conc_mask', 'head', 'concentration', 'model_name'\n",
    "    ]\n",
    "    \n",
    "    missing_cols = [col for col in required_cols if col not in cleaned_data.columns]\n",
    "    if missing_cols:\n",
    "        raise KeyError(f\"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}\")\n",
    "    \n",
    "  \n",
    "    M = cleaned_data['row'].max() + 1\n",
    "    N = cleaned_data['col'].max() + 1\n",
    "    T = cleaned_data['time_step'].max() + 1 - cleaned_data['time_step'].min()\n",
    "    \n",
    "    print(f\"ç½‘æ ¼å¤§å°: {M} x {N}, æ—¶é—´æ­¥æ•°: {T}\")\n",
    "    \n",
    "    unique_models = cleaned_data['model_name'].unique()\n",
    "    print(f\"æ€»æ¨¡å‹æ•°: {len(unique_models)}\")\n",
    "    \n",
    "  \n",
    "    train_models, val_models = train_test_split(unique_models, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train_data = cleaned_data[cleaned_data['model_name'].isin(train_models)]\n",
    "    val_data = cleaned_data[cleaned_data['model_name'].isin(val_models)]\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›†: {len(train_models)} ä¸ªæ¨¡å‹ ({len(train_models)/len(unique_models)*100:.1f}%)\")\n",
    "    print(f\"éªŒè¯é›†: {len(val_models)} ä¸ªæ¨¡å‹ ({len(val_models)/len(unique_models)*100:.1f}%)\")\n",
    "    \n",
    "  \n",
    "    train_dataset = HydroCNNDataset(train_data, (M, N), T)\n",
    "    val_dataset = HydroCNNDataset(val_data, (M, N), T)\n",
    "    \n",
    "\n",
    "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "        print(\"é”™è¯¯: æŸä¸ªæ•°æ®é›†ä¸ºç©ºï¼\")\n",
    "        print(f\"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
    "        print(f\"éªŒè¯é›†å¤§å°: {len(val_dataset)}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    \n",
    "    config = {\n",
    "        'hidden_dim': 96,        \n",
    "        'num_epochs': 500,\n",
    "        'lr': 1e-3,              \n",
    "        'weight_decay': 1e-4,\n",
    "        'patience': 30,\n",
    "        'save_path': './saved_models/cnn_dual_sequential',\n",
    "        'max_time_steps': T\n",
    "    }\n",
    "    \n",
    "    print(\"å¼€å§‹è®­ç»ƒCNNæ¨¡å‹...\")\n",
    "    print(f\"é…ç½®: {config}\")\n",
    "    \n",
    " \n",
    "    head_model, conc_model = train_dual_cnn(train_loader, val_loader, config)\n",
    "    \n",
    "  \n",
    "    print(\"\\nå¼€å§‹æœ€ç»ˆè¯„ä¼°ï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰...\")\n",
    "    head_metrics, conc_metrics = evaluate_dual_cnn(val_loader, config)\n",
    "    \n",
    "    print(\"\\nğŸ‰ CNNæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼\")\n",
    "    print(\"\\nğŸ“Š æœ€ç»ˆç»“æœæ€»ç»“:\")\n",
    "    print(f\"ğŸ“ˆ æ°´å¤´æ¨¡å‹ - R2: {head_metrics['r2']:.4f}, RMSE: {head_metrics['rmse']:.4f}\")\n",
    "    print(f\"ğŸ“ˆ æµ“åº¦æ¨¡å‹ - R2: {conc_metrics['r2']:.4f}, RMSE: {conc_metrics['rmse']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
