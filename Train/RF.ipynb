{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94e2b3-9c42-4d4c-9f66-5f3d73522f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf  \n",
    "from cuml.ensemble import RandomForestRegressor  \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "config = {\n",
    "    'num_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'random_state': 42,\n",
    "    'batch_size': 4,\n",
    "    'save_path': './saved_models/dual_guass_rf',\n",
    "    'num_epochs': 1,\n",
    "    'patience': 30,\n",
    "    'head_target_col': 'head',          \n",
    "    'conc_target_col': 'concentration'  \n",
    "}\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼Œæ­£ç¡®å¤„ç†cuDF/cuPyæ•°ç»„\"\"\"\n",
    "    \n",
    "\n",
    "    if hasattr(y_true, 'to_pandas'):\n",
    "        y_true = y_true.to_pandas().values\n",
    "    elif hasattr(y_true, 'get'):  \n",
    "        y_true = y_true.get()\n",
    "    elif hasattr(y_true, 'values'):\n",
    "        y_true = y_true.values\n",
    "    \n",
    "    if hasattr(y_pred, 'to_pandas'):\n",
    "        y_pred = y_pred.to_pandas().values\n",
    "    elif hasattr(y_pred, 'get'):  \n",
    "        y_pred = y_pred.get()\n",
    "    elif hasattr(y_pred, 'values'):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "    \n",
    "  \n",
    "    mask = ~np.isnan(y_true) & ~np.isinf(y_true) & ~np.isnan(y_pred) & ~np.isinf(y_pred)\n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    if len(y_true_clean) == 0:\n",
    "        return {'mse': np.inf, 'rmse': np.inf, 'mae': np.inf, 'r2': -np.inf}\n",
    "    \n",
    "    mse = mean_squared_error(y_true_clean, y_pred_clean)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "    r2 = r2_score(y_true_clean, y_pred_clean)\n",
    "    \n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "def preprocess_data_for_rf(data):\n",
    "\n",
    "    required_cols = [\n",
    "        'row', 'col', 'time_step', 'x', 'y', 'top', 'bottom', 'K', 'recharge', 'ET',\n",
    "        'river_stage', 'river_cond', 'river_rbot', 'well_rate', 'well_mask',\n",
    "        'chd_mask', 'lytyp', 'conc_mask', 'head', 'concentration'\n",
    "    ]\n",
    "    \n",
    "    missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        raise KeyError(f\"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}\")\n",
    "    \n",
    "    \n",
    "    data = data.astype({\n",
    "        'x': np.float32, 'y': np.float32, 'top': np.float32, \n",
    "        'bottom': np.float32, 'K': np.float32, 'recharge': np.float32,\n",
    "        'ET': np.float32, 'river_stage': np.float32, 'river_cond': np.float32,\n",
    "        'river_rbot': np.float32, 'well_rate': np.float32, 'well_mask': np.uint8,\n",
    "        'chd_mask': np.uint8, 'lytyp': np.uint8, 'head': np.float32, \n",
    "        'concentration': np.float32, 'conc_mask': np.uint8\n",
    "    })\n",
    "    \n",
    "    print(f\"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}\")\n",
    "    \n",
    " \n",
    "    data = data.dropna(subset=required_cols)\n",
    "    print(f\"å¤„ç†ç¼ºå¤±å€¼åæ•°æ®å½¢çŠ¶: {data.shape}\")\n",
    "    \n",
    "  \n",
    "    time_min = data['time_step'].min()\n",
    "    data['time_step'] = data['time_step'] - time_min\n",
    "    \n",
    " \n",
    "    data = data.sort_values(['row', 'col', 'time_step'])\n",
    "    \n",
    " \n",
    "    base_feature_cols = [\n",
    "        'x', 'y', 'top', 'bottom', 'K', 'recharge', 'ET',\n",
    "        'river_stage', 'river_cond', 'river_rbot', 'well_rate', 'well_mask',\n",
    "        'chd_mask', 'lytyp'\n",
    "    ]\n",
    "    \n",
    "   \n",
    "    conc_base_feature_cols = base_feature_cols + ['conc_mask']\n",
    "    \n",
    "    print(\"è®¡ç®—å†å²ç‰¹å¾...\")\n",
    "    \n",
    "  \n",
    "    prev_head = np.zeros(len(data), dtype=np.float32)\n",
    "    prev2_head = np.zeros(len(data), dtype=np.float32)\n",
    "    prev_conc = np.zeros(len(data), dtype=np.float32)\n",
    "    prev2_conc = np.zeros(len(data), dtype=np.float32)\n",
    "    \n",
    " \n",
    "    groups = data.groupby(['row', 'col'], sort=False)\n",
    "    for (row, col), group in groups:\n",
    "        time_series = group.sort_values('time_step')\n",
    "        indices = time_series.index\n",
    "        \n",
    "       \n",
    "        head_values = time_series['head'].values\n",
    "        conc_values = time_series['concentration'].values\n",
    "        \n",
    "        prev_head[indices] = np.roll(head_values, 1)\n",
    "        prev2_head[indices] = np.roll(head_values, 2)\n",
    "        prev_conc[indices] = np.roll(conc_values, 1)\n",
    "        prev2_conc[indices] = np.roll(conc_values, 2)\n",
    "        \n",
    " \n",
    "        first_idx = indices[0]\n",
    "        if len(indices) > 1:\n",
    "            second_idx = indices[1]\n",
    "         \n",
    "            prev_head[first_idx] = head_values[0]\n",
    "            prev2_head[first_idx] = head_values[0]\n",
    "            prev_conc[first_idx] = conc_values[0]\n",
    "            prev2_conc[first_idx] = conc_values[0]\n",
    "            \n",
    "            \n",
    "            prev2_head[second_idx] = head_values[0]\n",
    "            prev2_conc[second_idx] = conc_values[0]\n",
    "        else:\n",
    "            \n",
    "            prev_head[first_idx] = head_values[0]\n",
    "            prev2_head[first_idx] = head_values[0]\n",
    "            prev_conc[first_idx] = conc_values[0]\n",
    "            prev2_conc[first_idx] = conc_values[0]\n",
    "    \n",
    "\n",
    "    data['prev_head'] = prev_head\n",
    "    data['prev2_head'] = prev2_head\n",
    "    data['prev_conc'] = prev_conc\n",
    "    data['prev2_conc'] = prev2_conc\n",
    "    \n",
    "    print(\"å†å²ç‰¹å¾è®¡ç®—å®Œæˆ\")\n",
    "    \n",
    "\n",
    "    head_feature_cols = base_feature_cols + ['prev_head', 'prev2_head']\n",
    "    X_head = data[head_feature_cols].values.astype(np.float32)\n",
    "    \n",
    "\n",
    "    conc_feature_cols = conc_base_feature_cols + ['prev_head', 'prev2_head', 'prev_conc', 'prev2_conc']\n",
    "    X_conc = data[conc_feature_cols].values.astype(np.float32)\n",
    "    \n",
    "    print(f\"æ°´å¤´æ¨¡å‹ç‰¹å¾ç»´åº¦: {X_head.shape[1]} (æœŸæœ›16ç»´)\")\n",
    "    print(f\"æµ“åº¦æ¨¡å‹ç‰¹å¾ç»´åº¦: {X_conc.shape[1]} (æœŸæœ›19ç»´)\")\n",
    "    \n",
    "\n",
    "    def get_float_indices(feature_cols, data_dtypes):\n",
    "        float_indices = []\n",
    "        uint8_cols = ['well_mask', 'chd_mask', 'lytyp', 'conc_mask']\n",
    "        for i, col in enumerate(feature_cols):\n",
    "            if col not in uint8_cols:\n",
    "                float_indices.append(i)\n",
    "        return float_indices\n",
    "    \n",
    "    # æ°´å¤´æ¨¡å‹ç‰¹å¾æ ‡å‡†åŒ–\n",
    "    head_float_indices = get_float_indices(head_feature_cols, data.dtypes)\n",
    "    head_scaler = StandardScaler()\n",
    "    X_head_scaled = X_head.copy()\n",
    "    X_head_scaled[:, head_float_indices] = head_scaler.fit_transform(X_head[:, head_float_indices])\n",
    "    \n",
    " \n",
    "    conc_float_indices = get_float_indices(conc_feature_cols, data.dtypes)\n",
    "    conc_scaler = StandardScaler()\n",
    "    X_conc_scaled = X_conc.copy()\n",
    "    X_conc_scaled[:, conc_float_indices] = conc_scaler.fit_transform(X_conc[:, conc_float_indices])\n",
    "    \n",
    "\n",
    "    X_head_scaled = np.nan_to_num(X_head_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    X_conc_scaled = np.nan_to_num(X_conc_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "\n",
    "    y_head = data['head'].values.astype(np.float32)\n",
    "    y_conc = data['concentration'].values.astype(np.float32)\n",
    "    \n",
    "\n",
    "    index_data = data[['row', 'col', 'time_step']].copy()\n",
    "    \n",
    "    print(\"æ•°æ®é¢„å¤„ç†å®Œæˆ!\")\n",
    "    print(f\"æœ€ç»ˆç‰¹å¾å½¢çŠ¶ - æ°´å¤´: {X_head_scaled.shape}, æµ“åº¦: {X_conc_scaled.shape}\")\n",
    "    print(f\"ç›®æ ‡å˜é‡å½¢çŠ¶ - æ°´å¤´: {y_head.shape}, æµ“åº¦: {y_conc.shape}\")\n",
    "    \n",
    "    return (X_head_scaled, X_conc_scaled, y_head, y_conc, \n",
    "            head_scaler, conc_scaler, head_feature_cols, conc_feature_cols, index_data)\n",
    "\n",
    "def train_rf():\n",
    "  \n",
    "    cleaned_data = pd.read_csv('conc_dual_guass.csv')\n",
    "    \n",
    " \n",
    "    X_head, X_conc_base, y_head, y_conc, scaler_head, scaler_conc, head_feature_cols, conc_feature_cols, index_data = preprocess_data_for_rf(cleaned_data)\n",
    "    \n",
    "   \n",
    "    X_head_cudf = cudf.DataFrame(X_head)\n",
    "    y_head_cudf = cudf.Series(y_head)\n",
    "    y_conc_cudf = cudf.Series(y_conc)\n",
    "    \n",
    "   \n",
    "    print(\"åˆå§‹åŒ– 10 æŠ˜äº¤å‰éªŒè¯...\")\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=config['random_state'])\n",
    "    \n",
    "  \n",
    "    head_train_metrics_all = []\n",
    "    conc_train_metrics_all = []\n",
    "    head_test_metrics_all = []\n",
    "    conc_test_metrics_all = []\n",
    "    predictions_all = []\n",
    "    \n",
    "  \n",
    "    head_feature_importance = np.zeros(X_head.shape[1])\n",
    "    conc_feature_importance = np.zeros(X_conc_base.shape[1] + 1)  # +1 for predicted head\n",
    "    \n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_head_cudf)):\n",
    "        print(f\"\\nå¤„ç†ç¬¬ {fold + 1}/10 æŠ˜...\")\n",
    "        \n",
    "      \n",
    "        X_head_train = X_head_cudf.iloc[train_idx]\n",
    "        X_head_test = X_head_cudf.iloc[test_idx]\n",
    "        y_head_train = y_head_cudf.iloc[train_idx]\n",
    "        y_head_test = y_head_cudf.iloc[test_idx]\n",
    "        \n",
    " \n",
    "        X_conc_base_cudf = cudf.DataFrame(X_conc_base)\n",
    "        X_conc_train_base = X_conc_base_cudf.iloc[train_idx]\n",
    "        X_conc_test_base = X_conc_base_cudf.iloc[test_idx]\n",
    "        y_conc_train = y_conc_cudf.iloc[train_idx]\n",
    "        y_conc_test = y_conc_cudf.iloc[test_idx]\n",
    "        \n",
    "\n",
    "        print(f\"ç¬¬ {fold + 1} æŠ˜ - è®­ç»ƒé›†å½¢çŠ¶: X_head_train={X_head_train.shape}, y_head_train={len(y_head_train)}\")\n",
    "        print(f\"ç¬¬ {fold + 1} æŠ˜ - æµ‹è¯•é›†å½¢çŠ¶: X_head_test={X_head_test.shape}, y_head_test={len(y_head_test)}\")\n",
    "\n",
    "        print(f\"ç¬¬ {fold + 1} æŠ˜ - æ£€æŸ¥ X_head_train ä¸­çš„ NaN æˆ–æ— ç©·å€¼:\")\n",
    "        print(\"NaN:\", X_head_train.isnull().sum().sum())\n",
    "        print(\"æ— ç©·å€¼:\", (X_head_train == float('inf')).sum().sum() + (X_head_train == -float('inf')).sum().sum())\n",
    "        \n",
    "\n",
    "        head_rf = RandomForestRegressor(\n",
    "            n_estimators=config['num_estimators'],\n",
    "            max_depth=config['max_depth'],\n",
    "            random_state=config['random_state'],\n",
    "            n_streams=1\n",
    "        )\n",
    "        \n",
    "    \n",
    "        print(f\"ç¬¬ {fold + 1} æŠ˜ - è®­ç»ƒæ°´å¤´éšæœºæ£®æ—æ¨¡å‹...\")\n",
    "        head_rf.fit(X_head_train, y_head_train)\n",
    "        \n",
    "\n",
    "        try:\n",
    "           \n",
    "            head_importance = head_rf.get_feature_importances()\n",
    "            head_feature_importance += head_importance.to_numpy()\n",
    "            \n",
    "           \n",
    "            print(f\"ç¬¬ {fold + 1} æŠ˜ - æ°´å¤´æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ (å‰5):\")\n",
    "            head_importance_df = pd.DataFrame({\n",
    "                'feature': head_feature_cols,\n",
    "                'importance': head_importance.to_numpy()\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(head_importance_df.head(5))\n",
    "        except Exception as e:\n",
    "            print(f\"æ— æ³•è·å–æ°´å¤´æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§: {e}\")\n",
    "            print(\"è·³è¿‡ç‰¹å¾é‡è¦æ€§åˆ†æï¼Œç»§ç»­è®­ç»ƒ...\")\n",
    "        \n",
    "    \n",
    "        print(f\"ç¬¬ {fold + 1} æŠ˜ - å‡†å¤‡æµ“åº¦è®­ç»ƒæ•°æ®...\")\n",
    "        pred_head_train = head_rf.predict(X_head_train)\n",
    "        X_conc_train = X_conc_train_base.copy()\n",
    "        X_conc_train['pred_head'] = pred_head_train\n",
    "        \n",
    "  \n",
    "        print(f\"ç¬¬ {fold + 1} æŠ˜ - pred_head_train ä¸­çš„ NaN:\", pred_head_train.isnull().sum())\n",
    "        if X_conc_train['pred_head'].isnull().sum() > 0:\n",
    "            print(f\"ç¬¬ {fold + 1} æŠ˜ - è­¦å‘Š: X_conc_train['pred_head'] åŒ…å« {X_conc_train['pred_head'].isnull().sum()} ä¸ª NaN å€¼ï¼Œå¡«å……ä¸ºå‡å€¼\")\n",
    "            X_conc_train['pred_head'] = X_conc_train['pred_head'].fillna(X_conc_train['pred_head'].mean())\n",
    "        \n",
    "\n",
    "        conc_rf = RandomForestRegressor(\n",
    "            n_estimators=config['num_estimators'],\n",
    "            max_depth=config['max_depth'],\n",
    "            random_state=config['random_state'],\n",
    "            n_streams=1\n",
    "        )\n",
    "\n",
    "        print(f\"ç¬¬ {fold + 1} æŠ˜ - è®­ç»ƒæµ“åº¦éšæœºæ£®æ—æ¨¡å‹...\")\n",
    "        pred_head_test = head_rf.predict(X_head_test)\n",
    "        X_conc_test = X_conc_test_base.copy()\n",
    "        X_conc_test['pred_head'] = pred_head_test\n",
    "        \n",
    "        if X_conc_test['pred_head'].isnull().sum() > 0:\n",
    "            print(f\"ç¬¬ {fold + 1} æŠ˜ - è­¦å‘Š: X_conc_test['pred_head'] åŒ…å« {X_conc_test['pred_head'].isnull().sum()} ä¸ª NaN å€¼ï¼Œå¡«å……ä¸ºå‡å€¼\")\n",
    "            X_conc_test['pred_head'] = X_conc_test['pred_head'].fillna(X_conc_test['pred_head'].mean())\n",
    "        \n",
    "        conc_rf.fit(X_conc_train, y_conc_train)\n",
    "        \n",
    "\n",
    "        try:\n",
    "           \n",
    "            conc_importance = conc_rf.get_feature_importances()\n",
    "            conc_feature_importance += conc_importance.to_numpy()\n",
    "            \n",
    "         \n",
    "            print(f\"ç¬¬ {fold + 1} æŠ˜ - æµ“åº¦æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ (å‰5):\")\n",
    "            conc_feature_names = conc_feature_cols + ['pred_head']\n",
    "            conc_importance_df = pd.DataFrame({\n",
    "                'feature': conc_feature_names,\n",
    "                'importance': conc_importance.to_numpy()\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(conc_importance_df.head(5))\n",
    "        except Exception as e:\n",
    "            print(f\"æ— æ³•è·å–æµ“åº¦æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§: {e}\")\n",
    "            print(\"è·³è¿‡ç‰¹å¾é‡è¦æ€§åˆ†æï¼Œç»§ç»­è®­ç»ƒ...\")\n",
    "        \n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°\n",
    "        head_train_pred = head_rf.predict(X_head_train)\n",
    "        conc_train_pred = conc_rf.predict(X_conc_train)\n",
    "        \n",
    "        head_train_metrics = compute_metrics(y_head_train, head_train_pred)\n",
    "        conc_train_metrics = compute_metrics(y_conc_train, conc_train_pred)\n",
    "        head_train_metrics_all.append(head_train_metrics)\n",
    "        conc_train_metrics_all.append(conc_train_metrics)\n",
    "        \n",
    "     \n",
    "        head_test_pred = head_rf.predict(X_head_test)\n",
    "        conc_test_pred = conc_rf.predict(X_conc_test)\n",
    "        \n",
    "        head_test_metrics = compute_metrics(y_head_test, head_test_pred)\n",
    "        conc_test_metrics = compute_metrics(y_conc_test, conc_test_pred)\n",
    "        head_test_metrics_all.append(head_test_metrics)\n",
    "        conc_test_metrics_all.append(conc_test_metrics)\n",
    "        \n",
    "      \n",
    "        index_test = index_data.iloc[test_idx]\n",
    "        predictions = {\n",
    "            'fold': np.full(len(test_idx), fold + 1),\n",
    "            'row': index_test['row'].values,\n",
    "            'col': index_test['col'].values,\n",
    "            'time_step': index_test['time_step'].values,\n",
    "            'pred_conc': conc_test_pred.to_pandas().values,\n",
    "            'true_conc': y_conc_test.to_pandas().values,\n",
    "            'pred_head': head_test_pred.to_pandas().values,\n",
    "            'true_head': y_head_test.to_pandas().values\n",
    "        }\n",
    "        predictions_all.append(pd.DataFrame(predictions))\n",
    "    \n",
    " \n",
    "    os.makedirs(config['save_path'], exist_ok=True)\n",
    "    try:\n",
    "       \n",
    "        head_rf.save(os.path.join(config['save_path'], 'head_rf_model.pkl'))\n",
    "        conc_rf.save(os.path.join(config['save_path'], 'conc_rf_model.pkl'))\n",
    "    except Exception as e:\n",
    "        print(f\"æ— æ³•ä½¿ç”¨ cuML åŸç”Ÿæ–¹æ³•ä¿å­˜æ¨¡å‹: {e}\")\n",
    "        print(\"å°è¯•ä½¿ç”¨ pickle ä¿å­˜...\")\n",
    "        try:\n",
    "            with open(os.path.join(config['save_path'], 'head_rf_model.pkl'), 'wb') as f:\n",
    "                pickle.dump(head_rf, f)\n",
    "            with open(os.path.join(config['save_path'], 'conc_rf_model.pkl'), 'wb') as f:\n",
    "                pickle.dump(conc_rf, f)\n",
    "        except Exception as e2:\n",
    "            print(f\"æ— æ³•ä½¿ç”¨ pickle ä¿å­˜æ¨¡å‹: {e2}\")\n",
    "            print(\"æ¨¡å‹å°†ä¸ä¼šè¢«ä¿å­˜ã€‚\")\n",
    "    \n",
    "\n",
    "    with open(os.path.join(config['save_path'], 'scaler_head.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler_head, f)\n",
    "    with open(os.path.join(config['save_path'], 'scaler_conc.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler_conc, f)\n",
    "    \n",
    " \n",
    "    predictions_df = pd.concat(predictions_all, ignore_index=True)\n",
    "    predictions_df.to_csv(os.path.join(config['save_path'], 'test_predictions_rf_kfold.csv'), index=False)\n",
    "    \n",
    "\n",
    "    try:\n",
    "     \n",
    "        head_feature_importance /= 10  \n",
    "        conc_feature_importance /= 10  \n",
    "        \n",
    "       \n",
    "        head_importance_final = pd.DataFrame({\n",
    "            'feature': head_feature_cols,\n",
    "            'importance': head_feature_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        conc_importance_final = pd.DataFrame({\n",
    "            'feature': conc_feature_cols + ['pred_head'],\n",
    "            'importance': conc_feature_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        head_importance_final.to_csv(os.path.join(config['save_path'], 'head_feature_importance.csv'), index=False)\n",
    "        conc_importance_final.to_csv(os.path.join(config['save_path'], 'conc_feature_importance.csv'), index=False)\n",
    "        \n",
    "  \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "       \n",
    "        plt.subplot(2, 1, 1)\n",
    "        head_top10 = head_importance_final.head(10)\n",
    "        plt.barh(head_top10['feature'], head_top10['importance'])\n",
    "        plt.title('æ°´å¤´æ¨¡å‹ç‰¹å¾é‡è¦æ€§ (å‰10)')\n",
    "        plt.xlabel('é‡è¦æ€§')\n",
    "        plt.gca().invert_yaxis()  \n",
    "        \n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        conc_top10 = conc_importance_final.head(10)\n",
    "        plt.barh(conc_top10['feature'], conc_top10['importance'])\n",
    "        plt.title('æµ“åº¦æ¨¡å‹ç‰¹å¾é‡è¦æ€§ (å‰10)')\n",
    "        plt.xlabel('é‡è¦æ€§')\n",
    "        plt.gca().invert_yaxis()  \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config['save_path'], 'feature_importance.png'), dpi=300)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"æ— æ³•ä¿å­˜æˆ–ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§: {e}\")\n",
    "        print(\"è·³è¿‡ç‰¹å¾é‡è¦æ€§åˆ†æï¼Œç»§ç»­è¯„ä¼°...\")\n",
    "    \n",
    "\n",
    "    def average_metrics(metrics_list):\n",
    "        keys = metrics_list[0].keys()\n",
    "        avg_metrics = {key: np.mean([m[key] for m in metrics_list]) for key in keys}\n",
    "        return avg_metrics\n",
    "    \n",
    "    avg_head_train_metrics = average_metrics(head_train_metrics_all)\n",
    "    avg_conc_train_metrics = average_metrics(conc_train_metrics_all)\n",
    "    avg_head_test_metrics = average_metrics(head_test_metrics_all)\n",
    "    avg_conc_test_metrics = average_metrics(conc_test_metrics_all)\n",
    "    \n",
    "\n",
    "    print(\"\\nğŸ“Š å¹³å‡è®­ç»ƒé›†ç»“æœ (10æŠ˜äº¤å‰éªŒè¯):\")\n",
    "    print(\"\\nğŸ”¹ æ°´å¤´æŒ‡æ ‡:\")\n",
    "    for k, v in avg_head_train_metrics.items():\n",
    "        print(f\"{k.upper():<5}: {v:.4f}\")\n",
    "    print(\"\\nğŸ”¹ æµ“åº¦æŒ‡æ ‡:\")\n",
    "    for k, v in avg_conc_train_metrics.items():\n",
    "        print(f\"{k.upper():<5}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š å¹³å‡æµ‹è¯•é›†ç»“æœ (10æŠ˜äº¤å‰éªŒè¯):\")\n",
    "    print(\"\\nğŸ”¹ æ°´å¤´æŒ‡æ ‡:\")\n",
    "    for k, v in avg_head_test_metrics.items():\n",
    "        print(f\"{k.upper():<5}: {v:.4f}\")\n",
    "    print(\"\\nğŸ”¹ æµ“åº¦æŒ‡æ ‡:\")\n",
    "    for k, v in avg_conc_test_metrics.items():\n",
    "        print(f\"{k.upper():<5}: {v:.4f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶é¢„æµ‹ä¸çœŸå®å€¼å¯¹æ¯”å›¾\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    " \n",
    "        plt.subplot(2, 1, 1)\n",
    "        all_pred_head = np.concatenate([df['pred_head'].values for df in predictions_all])\n",
    "        all_true_head = np.concatenate([df['true_head'].values for df in predictions_all])\n",
    "        plt.scatter(all_true_head, all_pred_head, alpha=0.3)\n",
    "        min_val = min(all_true_head.min(), all_pred_head.min())\n",
    "        max_val = max(all_true_head.max(), all_pred_head.max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "        plt.title(f'æ°´å¤´é¢„æµ‹ vs çœŸå®å€¼ (RÂ² = {avg_head_test_metrics[\"r2\"]:.4f})')\n",
    "        plt.xlabel('çœŸå®æ°´å¤´')\n",
    "        plt.ylabel('é¢„æµ‹æ°´å¤´')\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        all_pred_conc = np.concatenate([df['pred_conc'].values for df in predictions_all])\n",
    "        all_true_conc = np.concatenate([df['true_conc'].values for df in predictions_all])\n",
    "        plt.scatter(all_true_conc, all_pred_conc, alpha=0.3)\n",
    "        min_val = min(all_true_conc.min(), all_pred_conc.min())\n",
    "        max_val = max(all_true_conc.max(), all_pred_conc.max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "        plt.title(f'æµ“åº¦é¢„æµ‹ vs çœŸå®å€¼ (RÂ² = {avg_conc_test_metrics[\"r2\"]:.4f})')\n",
    "        plt.xlabel('çœŸå®æµ“åº¦')\n",
    "        plt.ylabel('é¢„æµ‹æµ“åº¦')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config['save_path'], 'predictions_vs_truth.png'), dpi=300)\n",
    "    except Exception as e:\n",
    "        print(f\"æ— æ³•ç»˜åˆ¶é¢„æµ‹å›¾è¡¨: {e}\")\n",
    "        print(\"è·³è¿‡å›¾è¡¨ç»˜åˆ¶ï¼Œç»§ç»­è¿”å›ç»“æœ...\")\n",
    "    \n",
    "    return avg_head_train_metrics, avg_conc_train_metrics, avg_head_test_metrics, avg_conc_test_metrics\n",
    "\n",
    "\n",
    "avg_head_train_metrics, avg_conc_train_metrics, avg_head_test_metrics, avg_conc_test_metrics = train_rf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
