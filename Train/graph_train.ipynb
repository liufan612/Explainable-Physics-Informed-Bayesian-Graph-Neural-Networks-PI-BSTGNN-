{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f3155-321e-498b-9c3c-8ab560405134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from sklearn.model_selection import train_test_split                                                                                                   \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.amp import autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from sklearn.metrics import r2_score as sklearn_r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/GNO/GNN/GNNShap')\n",
    "from gnnshap.explainer import GNNShapExplainer\n",
    "import uuid\n",
    "\n",
    "\n",
    "from blitz.modules import BayesianLinear                 \n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class HydroDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        self.graphs = graphs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "class BlitzSTConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    使用Blitz实现的贝叶斯时空卷积层，增强鲁棒性\n",
    "    \"\"\"\n",
    "    def __init__(self, spatial_dim, prior_sigma_1=0.1, prior_sigma_2=0.002, posterior_mu_init=0.0, posterior_rho_init=-3.0, dropout=0.1):\n",
    "        super().__init__(aggr='mean')\n",
    "\n",
    "        self.pre_msg = nn.Linear(2 * spatial_dim + 3, spatial_dim)\n",
    "        \n",
    "\n",
    "        self.bayes_msg = nn.Sequential(\n",
    "            BayesianLinear(spatial_dim, spatial_dim,\n",
    "                        prior_sigma_1=prior_sigma_1, \n",
    "                        prior_sigma_2=prior_sigma_2,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LayerNorm(spatial_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.pre_gate = nn.Linear(3 * spatial_dim, spatial_dim)\n",
    "        self.bayes_gate = nn.Sequential(\n",
    "            BayesianLinear(spatial_dim, spatial_dim,\n",
    "                        prior_sigma_1=prior_sigma_1, \n",
    "                        prior_sigma_2=prior_sigma_2,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.pre_res = nn.Linear(spatial_dim, spatial_dim)\n",
    "        self.bayes_res = BayesianLinear(spatial_dim, spatial_dim,\n",
    "                                     prior_sigma_1=prior_sigma_1/2, \n",
    "                                     prior_sigma_2=prior_sigma_2/2,\n",
    "                                     posterior_mu_init=posterior_mu_init,\n",
    "                                     posterior_rho_init=posterior_rho_init)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        try:\n",
    "\n",
    "            edge_attr = edge_attr.float()\n",
    "            \n",
    "\n",
    "            out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "            \n",
    "\n",
    "            combined = torch.cat([x, out, x - out], dim=-1)\n",
    "            gate_pre = self.pre_gate(combined)\n",
    "            gate = self.bayes_gate(gate_pre)\n",
    "            \n",
    "\n",
    "            res_pre = self.pre_res(x)\n",
    "            res = self.bayes_res(res_pre)\n",
    "            \n",
    "            return x + gate * out + 0.1 * res\n",
    "        except Exception as e:\n",
    "            print(f\"Error in BlitzSTConv.forward: {e}\")\n",
    "\n",
    "            print(f\"x shape: {x.shape}, dtype: {x.dtype}\")\n",
    "            print(f\"edge_index shape: {edge_index.shape}, dtype: {edge_index.dtype}\")\n",
    "            print(f\"edge_attr shape: {edge_attr.shape}, dtype: {edge_attr.dtype}\")\n",
    "            raise\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        try:\n",
    "\n",
    "            edge_attr = edge_attr.to(x_i.dtype).to(x_i.device)\n",
    "            \n",
    "\n",
    "            combined = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
    "            pre_msg = self.pre_msg(combined)\n",
    "            return self.bayes_msg(pre_msg)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in BlitzSTConv.message: {e}\")\n",
    "\n",
    "            print(f\"x_i shape: {x_i.shape}, dtype: {x_i.dtype}\")\n",
    "            print(f\"x_j shape: {x_j.shape}, dtype: {x_j.dtype}\")\n",
    "            print(f\"edge_attr shape: {edge_attr.shape}, dtype: {edge_attr.dtype}\")\n",
    "            raise\n",
    "\n",
    "class BlitzBoundaryProcessor(nn.Module):\n",
    "    \"\"\"\n",
    "    使用Blitz实现的贝叶斯边界处理器\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, prior_sigma_1=0.1, prior_sigma_2=0.002, posterior_mu_init=0.0, posterior_rho_init=-3.0):\n",
    "        super().__init__()\n",
    "        self.boundary_net = nn.Sequential(\n",
    "            BayesianLinear(dim + 1, dim,\n",
    "                        prior_sigma_1=prior_sigma_1, \n",
    "                        prior_sigma_2=prior_sigma_2,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.river_net = nn.Sequential(\n",
    "            BayesianLinear(dim + 2, dim,\n",
    "                        prior_sigma_1=prior_sigma_1, \n",
    "                        prior_sigma_2=prior_sigma_2,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.well_net = BayesianLinear(dim + 1, dim,\n",
    "                                    prior_sigma_1=prior_sigma_1, \n",
    "                                    prior_sigma_2=prior_sigma_2,\n",
    "                                    posterior_mu_init=posterior_mu_init,\n",
    "                                    posterior_rho_init=posterior_rho_init)\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            BayesianLinear(2 * dim, dim,\n",
    "                        prior_sigma_1=prior_sigma_1/2, \n",
    "                        prior_sigma_2=prior_sigma_2/2,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.chd_enforcer = BayesianLinear(dim, dim,\n",
    "                                        prior_sigma_1=prior_sigma_1/2, \n",
    "                                        prior_sigma_2=prior_sigma_2/2,\n",
    "                                        posterior_mu_init=posterior_mu_init,\n",
    "                                        posterior_rho_init=posterior_rho_init)\n",
    "    \n",
    "    def forward(self, x, bc_mask):\n",
    "        boundary_feat = self.boundary_net(\n",
    "            torch.cat([x, bc_mask[:, 0:1]], dim=-1)\n",
    "        ) * bc_mask[:, 0:1]\n",
    "        \n",
    "        river_feat = self.river_net(\n",
    "            torch.cat([x, bc_mask[:, 1:3]], dim=-1)\n",
    "        ) * bc_mask[:, 1:2]\n",
    "        \n",
    "        well_feat = self.well_net(\n",
    "            torch.cat([x, bc_mask[:, 3:4]], dim=-1)\n",
    "        ) * bc_mask[:, 4:5]\n",
    "        \n",
    "        combined = boundary_feat + river_feat + well_feat\n",
    "        gate = self.gate(torch.cat([x, combined], dim=-1))\n",
    "        out = x * (1 - gate) + combined * gate\n",
    "        \n",
    "        chd_mask = bc_mask[:, 0] > 0\n",
    "        if chd_mask.sum() > 0:\n",
    "            chd_out = self.chd_enforcer(out[chd_mask]).to(out.dtype)\n",
    "            out[chd_mask] = chd_out\n",
    "            \n",
    "        return out\n",
    "\n",
    "@variational_estimator\n",
    "class BlitzHeadGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, node_features=16, max_time_steps=40, spatial_dim=64, \n",
    "                temporal_dim=64, output_dim=1, prior_sigma_1=0.05, prior_sigma_2=0.001,\n",
    "                posterior_mu_init=0.0, posterior_rho_init=-3.0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.spatial_dim = spatial_dim\n",
    "        self.time_embed = nn.Embedding(max_time_steps + 1, temporal_dim)\n",
    "        \n",
    "\n",
    "        self.node_enc = nn.Sequential(\n",
    "            nn.Linear(node_features + temporal_dim, spatial_dim),\n",
    "            nn.BatchNorm1d(spatial_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(spatial_dim, spatial_dim),\n",
    "            nn.BatchNorm1d(spatial_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            BlitzSTConv(spatial_dim, prior_sigma_1, prior_sigma_2, \n",
    "                     posterior_mu_init, posterior_rho_init, dropout) \n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "\n",
    "        self.bc_processor = BlitzBoundaryProcessor(\n",
    "            spatial_dim, prior_sigma_1, prior_sigma_2, \n",
    "            posterior_mu_init, posterior_rho_init\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.deterministic_path = nn.Sequential(\n",
    "            nn.Linear(spatial_dim, spatial_dim // 2),\n",
    "            nn.BatchNorm1d(spatial_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(spatial_dim // 2, spatial_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(spatial_dim // 4, output_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.bayesian_path = nn.Sequential(\n",
    "            BayesianLinear(spatial_dim, spatial_dim // 2,\n",
    "                          prior_sigma_1=prior_sigma_1, \n",
    "                          prior_sigma_2=prior_sigma_2,\n",
    "                          posterior_mu_init=posterior_mu_init,\n",
    "                          posterior_rho_init=posterior_rho_init),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            BayesianLinear(spatial_dim // 2, output_dim,\n",
    "                          prior_sigma_1=prior_sigma_1/2, \n",
    "                          prior_sigma_2=prior_sigma_2/2,\n",
    "                          posterior_mu_init=posterior_mu_init,\n",
    "                          posterior_rho_init=posterior_rho_init),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(spatial_dim, spatial_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(spatial_dim // 4, spatial_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr.to(torch.float32)\n",
    "        \n",
    "\n",
    "        if hasattr(self, 'feature_engineering'):\n",
    "            x = self.feature_engineering(x)\n",
    "        \n",
    "\n",
    "        time_emb = self.time_embed(data.time_step)\n",
    "        node_feat = torch.cat([x, time_emb], dim=-1)\n",
    "\n",
    "        h = self.node_enc(node_feat)\n",
    "        \n",
    "\n",
    "        for conv in self.conv_layers:\n",
    "            h_new = conv(h, edge_index, edge_attr)\n",
    "            h = h + 0.1 * h_new\n",
    "        \n",
    "\n",
    "        attention_weights = self.attention(h)\n",
    "        h = h * attention_weights\n",
    "        \n",
    "  \n",
    "        h = self.bc_processor(h, data.bc_mask)\n",
    "        \n",
    "\n",
    "        det_pred = self.deterministic_path(h.detach())\n",
    "        bayes_pred = self.bayesian_path(h)\n",
    "        \n",
    "\n",
    "        return det_pred * 0.7 + bayes_pred * 0.3\n",
    "\n",
    "class ImprovedPhysicsInformedLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=0.5, kl_weight=1e-4):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.kl_weight = kl_weight  \n",
    "        \n",
    "    def forward(self, pred, data, model=None):  \n",
    "\n",
    "        mse_loss = F.mse_loss(pred, data.head_y.unsqueeze(1))\n",
    "\n",
    "        time_steps = data.time_step.unique(sorted=True)\n",
    "        flux_loss = 0\n",
    "        for t in time_steps[:-1]:\n",
    "            mask_t = (data.time_step == t)\n",
    "            mask_next = (data.time_step == t + 1)\n",
    "            if mask_t.sum() > 0 and mask_next.sum() > 0:\n",
    "                flux_diff = torch.mean((pred[mask_next] - pred[mask_t]) ** 2)\n",
    "                flux_loss += flux_diff\n",
    "        flux_loss /= len(time_steps) - 1 if len(time_steps) > 1 else 1\n",
    "        \n",
    "\n",
    "        bc_mask = data.bc_mask[:, 0] > 0\n",
    "        bc_loss = F.l1_loss(pred[bc_mask], data.head_y[bc_mask].unsqueeze(1)) if bc_mask.sum() > 0 else torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "\n",
    "        well_mask = data.bc_mask[:, 4] > 0\n",
    "        well_loss = F.l1_loss(pred[well_mask], data.head_y[well_mask].unsqueeze(1)) if well_mask.sum() > 0 else torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "\n",
    "        total_loss = (1 - self.alpha) * mse_loss + self.alpha * (flux_loss + bc_loss + well_loss)\n",
    "        \n",
    "        return total_loss, (mse_loss.item(), flux_loss.item(), bc_loss.item(), well_loss.item(), 0.0) \n",
    "\n",
    "\n",
    "@variational_estimator\n",
    "class BlitzConcGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, node_features=19, max_time_steps=40, spatial_dim=128,\n",
    "                temporal_dim=64, output_dim=1, prior_sigma_1=0.1, prior_sigma_2=0.01,\n",
    "                posterior_mu_init=0.0, posterior_rho_init=-3.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.spatial_dim = spatial_dim\n",
    "        self.time_embed = nn.Embedding(max_time_steps + 1, temporal_dim)\n",
    "        \n",
    "\n",
    "        self.node_enc_scale1 = nn.Sequential(\n",
    "            nn.Linear(node_features + temporal_dim, spatial_dim),  # 18 + 64 = 82 -> 128\n",
    "            nn.BatchNorm1d(spatial_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        self.node_enc = nn.Sequential(\n",
    "            BayesianLinear(spatial_dim, spatial_dim,\n",
    "                        prior_sigma_1=prior_sigma_1, \n",
    "                        prior_sigma_2=prior_sigma_2,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LayerNorm(spatial_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            BlitzSTConv(spatial_dim, prior_sigma_1, prior_sigma_2, \n",
    "                     posterior_mu_init, posterior_rho_init, dropout) \n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "\n",
    "        self.bc_processor = BlitzBoundaryProcessor(\n",
    "            spatial_dim, prior_sigma_1, prior_sigma_2, \n",
    "            posterior_mu_init, posterior_rho_init\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(spatial_dim, spatial_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(spatial_dim // 4, spatial_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            BayesianLinear(spatial_dim, 128,\n",
    "                        prior_sigma_1=prior_sigma_1/2, \n",
    "                        prior_sigma_2=prior_sigma_2/2,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            BayesianLinear(128, 64,\n",
    "                        prior_sigma_1=prior_sigma_1/2, \n",
    "                        prior_sigma_2=prior_sigma_2/2,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(64, output_dim,\n",
    "                        prior_sigma_1=prior_sigma_1/4, \n",
    "                        prior_sigma_2=prior_sigma_2/4,\n",
    "                        posterior_mu_init=posterior_mu_init,\n",
    "                        posterior_rho_init=posterior_rho_init),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.decoder_det = nn.Sequential(\n",
    "            nn.Linear(spatial_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.branch_weight = nn.Parameter(torch.tensor(0.3))\n",
    "\n",
    "    def forward(self, data, pred_head=None):\n",
    "\n",
    "        x = data.conc_x\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        \n",
    "\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = edge_attr.to(torch.float32).to(x.device)\n",
    "        \n",
    "\n",
    "        time_step = data.time_step.to(x.device)\n",
    "        time_emb = self.time_embed(time_step)\n",
    "\n",
    "        node_feat = torch.cat([x, time_emb], dim=-1)  \n",
    "        \n",
    "\n",
    "        h_scale1 = self.node_enc_scale1(node_feat)\n",
    "        h = self.node_enc(h_scale1)\n",
    "        \n",
    "\n",
    "        for conv in self.conv_layers:\n",
    "            h_new = conv(h, edge_index, edge_attr)\n",
    "            h = h + 0.1 * h_new  \n",
    "        \n",
    "\n",
    "        attention_weights = self.attention(h)\n",
    "        h = h * attention_weights\n",
    "        \n",
    "\n",
    "        bc_mask = data.bc_mask.to(x.device) if hasattr(data, 'bc_mask') else None\n",
    "        h = self.bc_processor(h, bc_mask)\n",
    "        \n",
    "\n",
    "        bayes_output = self.decoder(h)\n",
    "        det_output = self.decoder_det(h.detach())\n",
    "        \n",
    "\n",
    "        combined_output = (\n",
    "            torch.sigmoid(self.branch_weight) * bayes_output + \n",
    "            (1 - torch.sigmoid(self.branch_weight)) * det_output\n",
    "        )\n",
    "        \n",
    "        return combined_output\n",
    "\n",
    "class ImprovedConcLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, kl_weight=5e-5, l1_weight=1e-8): \n",
    "        super().__init__()\n",
    "        self.kl_weight = kl_weight\n",
    "        self.l1_weight = l1_weight \n",
    "        \n",
    "    def forward(self, pred, data, model=None):\n",
    "\n",
    "        mse_loss = F.mse_loss(pred, data.y.unsqueeze(1))\n",
    "        \n",
    "\n",
    "        l1_reg = torch.tensor(0., device=pred.device)\n",
    "        if model is not None and self.l1_weight > 0:\n",
    "            for name, param in model.named_parameters():\n",
    "\n",
    "                if 'weight' in name and ('bayesian' in name.lower() or 'bayes' in name.lower()):\n",
    "                    l1_reg += torch.norm(param, 1)\n",
    "        \n",
    "\n",
    "        total_loss = mse_loss + self.l1_weight * l1_reg\n",
    "        \n",
    "        return total_loss, (mse_loss.item(), 0.0, l1_reg.item())\n",
    "\n",
    "def generate_spatial_edges(model_df):\n",
    "\n",
    "    spatial_edges = []\n",
    "    time_steps = model_df['time_step'].unique()\n",
    "    for t in time_steps:\n",
    "        time_df = model_df[model_df['time_step'] == t]\n",
    "        coord_to_idx = {(r, c): idx for idx, r, c in zip(time_df['local_index'], time_df['row'], time_df['col'])}\n",
    "        for idx, row, col in zip(time_df['local_index'], time_df['row'], time_df['col']):\n",
    "            right_coord = (row, col + 1)\n",
    "            if right_coord in coord_to_idx:\n",
    "                spatial_edges.append([idx, coord_to_idx[right_coord]])\n",
    "            upper_coord = (row + 1, col)\n",
    "            if upper_coord in coord_to_idx:\n",
    "                spatial_edges.append([idx, coord_to_idx[upper_coord]])\n",
    "    return np.array(spatial_edges), np.full((len(spatial_edges), 3), [1.0, 0, 0], dtype=np.float32)\n",
    "\n",
    "def generate_temporal_edges(model_df):\n",
    "\n",
    "    temporal_edges = []\n",
    "    groups = model_df.groupby(['row', 'col'], sort=False)\n",
    "    for (row, col), group in groups:\n",
    "        time_series = group.sort_values('time_step')\n",
    "        for i in range(len(time_series) - 1):\n",
    "            global_src = time_series['local_index'].iloc[i]\n",
    "            global_dst = time_series['local_index'].iloc[i + 1]\n",
    "            temporal_edges.append([global_src, global_dst])\n",
    "    return np.array(temporal_edges), np.full((len(temporal_edges), 3), [0.0, 1.0, 0], dtype=np.float32)\n",
    "\n",
    "def build_bc_mask(model_df):\n",
    "\n",
    "    bc_mask = np.zeros((len(model_df), 5), dtype=np.float32)\n",
    "    bc_mask[:, 0] = model_df['chd_mask'].values.astype(np.float32)\n",
    "    bc_mask[:, 1] = (model_df['river_cond'] > 0).astype(np.float32)\n",
    "    bc_mask[:, 2] = model_df['river_stage'].values.astype(np.float32)\n",
    "    bc_mask[:, 3] = model_df['well_rate'].values.astype(np.float32)\n",
    "    bc_mask[:, 4] = model_df['well_mask'].values.astype(np.float32)\n",
    "    return bc_mask\n",
    "def build_spatiotemporal_graph(df):\n",
    "\n",
    "    print(f\"\\n▶ Started building spatiotemporal graphs\")\n",
    "    print(f\"▷ Total models to process: {len(df['model_name'].unique())}\")\n",
    "    graphs = []\n",
    "    \n",
    "\n",
    "    df = df.astype({\n",
    "        'x': np.float32, 'y': np.float32, 'top': np.float32, \n",
    "        'bottom': np.float32, 'K': np.float32, 'recharge': np.float32,\n",
    "        'ET': np.float32, 'river_stage': np.float32, 'river_cond': np.float32,\n",
    "        'river_rbot': np.float32, 'well_rate': np.float32, 'well_mask': np.uint8,\n",
    "        'chd_mask': np.uint8, 'lytyp': np.uint8, 'head': np.float32, \n",
    "        'concentration': np.float32,'conc_mask': np.uint8\n",
    "    })\n",
    "    \n",
    "    time_min = df['time_step'].min()\n",
    "    df['time_step'] = df['time_step'] - time_min\n",
    "    \n",
    "    model_groups = list(df.groupby('model_name', sort=False))\n",
    "    total_models = len(model_groups)\n",
    "    \n",
    "    for model_idx, (model_name, model_df) in enumerate(model_groups, 1):\n",
    "        model_df = model_df.reset_index(drop=True).copy()\n",
    "        model_df['local_index'] = model_df.index\n",
    "        print(f\"\\n▣ Processing model {model_idx}/{total_models}: {model_name}\")\n",
    "        \n",
    "        model_df = model_df.sort_values(['row', 'col', 'time_step'])\n",
    "        \n",
    "\n",
    "        feature_cols = [\n",
    "            'x', 'y', 'top', 'bottom', 'K', 'recharge', 'ET',\n",
    "            'river_stage', 'river_cond', 'river_rbot', 'well_rate', 'well_mask',\n",
    "            'chd_mask', 'lytyp'\n",
    "        ]\n",
    "        node_feats = model_df[feature_cols].values.astype(np.float32)\n",
    "        col_types = df[feature_cols].dtypes.to_dict()\n",
    "        float_indices = [i for i, col in enumerate(feature_cols) if col_types[col] != np.uint8]\n",
    "        float_feats = node_feats[:, float_indices]\n",
    "        scaler = StandardScaler()\n",
    "        float_feats_scaled = scaler.fit_transform(float_feats)\n",
    "        node_feats[:, float_indices] = float_feats_scaled\n",
    "        conc_feature_cols = [\n",
    "            'x', 'y', 'top', 'bottom', 'K', 'recharge', 'ET',\n",
    "            'river_stage', 'river_cond', 'river_rbot', 'well_rate', 'well_mask',\n",
    "            'chd_mask', 'lytyp','conc_mask'\n",
    "        ]\n",
    "        conc_node_feats = model_df[conc_feature_cols].values.astype(np.float32)\n",
    "        col_types = df[conc_feature_cols].dtypes.to_dict()\n",
    "        float_indices = [i for i, col in enumerate(conc_feature_cols) if col_types[col] != np.uint8]\n",
    "        conc_float_feats = conc_node_feats[:, float_indices]\n",
    "        scaler = StandardScaler()\n",
    "        conc_float_feats_scaled = scaler.fit_transform(conc_float_feats)\n",
    "        conc_node_feats[:, float_indices] = conc_float_feats_scaled\n",
    "\n",
    "        prev_head = np.zeros(len(model_df), dtype=np.float32)\n",
    "        prev2_head = np.zeros(len(model_df), dtype=np.float32)\n",
    "        prev_conc = np.zeros(len(model_df), dtype=np.float32)\n",
    "        prev2_conc = np.zeros(len(model_df), dtype=np.float32)\n",
    "        \n",
    "        groups = model_df.groupby(['row', 'col'], sort=False)\n",
    "        for (row, col), group in groups:\n",
    "            time_series = group.sort_values('time_step')\n",
    "            prev_head[time_series.index] = np.roll(time_series['head'].values, 1)\n",
    "            prev2_head[time_series.index] = np.roll(time_series['head'].values, 2)\n",
    "            prev_conc[time_series.index] = np.roll(time_series['concentration'].values, 1)\n",
    "            prev2_conc[time_series.index] = np.roll(time_series['concentration'].values, 2)\n",
    "            \n",
    "            first_idx = time_series.index[0]\n",
    "            if len(time_series) > 1:\n",
    "                second_idx = time_series.index[1]\n",
    "\n",
    "                prev_head[first_idx] = time_series['head'].values[0]\n",
    "                prev2_head[first_idx] = time_series['head'].values[0]\n",
    "                prev2_head[second_idx] = time_series['head'].values[0]\n",
    "                \n",
    "\n",
    "                prev_conc[first_idx] = time_series['concentration'].values[0]\n",
    "                prev2_conc[first_idx] = time_series['concentration'].values[0]\n",
    "                prev2_conc[second_idx] = time_series['concentration'].values[0]\n",
    "            else:\n",
    "                prev_head[first_idx] = 0.0\n",
    "                prev2_head[first_idx] = 0.0\n",
    "                prev_conc[first_idx] = 0.0\n",
    "                prev2_conc[first_idx] = 0.0\n",
    "\n",
    "\n",
    "        head_feats = np.concatenate([\n",
    "            node_feats,          \n",
    "            prev_head[:, None],  \n",
    "            prev2_head[:, None]   \n",
    "        ], axis=1)\n",
    "        \n",
    "\n",
    "        conc_feats = np.concatenate([\n",
    "            conc_node_feats,         \n",
    "            prev_head[:, None],   \n",
    "            prev2_head[:, None],  \n",
    "            prev_conc[:, None],  \n",
    "            prev2_conc[:, None]   \n",
    "        ], axis=1)\n",
    "        \n",
    "        if np.any(np.isnan(head_feats)) or np.any(np.isinf(head_feats)):\n",
    "            print(f\"Warning: head_feats contains NaN or Inf for model {model_name}\")\n",
    "            head_feats = np.nan_to_num(head_feats, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        \n",
    "        if np.any(np.isnan(conc_feats)) or np.any(np.isinf(conc_feats)):\n",
    "            print(f\"Warning: conc_feats contains NaN or Inf for model {model_name}\")\n",
    "            conc_feats = np.nan_to_num(conc_feats, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        \n",
    "        conc = model_df['concentration'].values.astype(np.float32)\n",
    "        head = model_df['head'].values.astype(np.float32)\n",
    "        spatial_edges, spatial_attrs = generate_spatial_edges(model_df)\n",
    "        temporal_edges, temporal_attrs = generate_temporal_edges(model_df)\n",
    "        edges = np.concatenate([spatial_edges, temporal_edges], axis=0)\n",
    "        edge_attr = np.concatenate([spatial_attrs, temporal_attrs], axis=0)\n",
    "        bc_mask = build_bc_mask(model_df)\n",
    "        \n",
    "        assert bc_mask.shape == (len(model_df), 5), \\\n",
    "            f\"Invalid bc_mask shape: {bc_mask.shape} for model {model_name}\"\n",
    "        \n",
    "        graph = Data(\n",
    "            x=torch.from_numpy(head_feats),      \n",
    "            conc_x=torch.from_numpy(conc_feats),\n",
    "            edge_index=torch.tensor(edges.T, dtype=torch.long),\n",
    "            edge_attr=torch.from_numpy(edge_attr),\n",
    "            y=torch.from_numpy(conc),\n",
    "            head_y=torch.from_numpy(head),\n",
    "            bc_mask=torch.from_numpy(bc_mask),\n",
    "            time_step=torch.from_numpy(model_df['time_step'].values).long(),\n",
    "            time_steps=model_df['time_step'].nunique(),\n",
    "            model_name=str(model_name),\n",
    "            row=torch.from_numpy(model_df['row'].values).long(),\n",
    "            col=torch.from_numpy(model_df['col'].values).long(),\n",
    "        )\n",
    "        graphs.append(graph)\n",
    "    \n",
    "    print(f\"\\n✅ All models processed! Total graphs created: {len(graphs):,}\")\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def prepare_data(data, batch_size=4):\n",
    "\n",
    "    print('正在处理数据...')\n",
    "    all_graphs = build_spatiotemporal_graph(data)\n",
    "    print('数据处理完成！')\n",
    "    train_graphs, val_graphs = train_test_split(\n",
    "        all_graphs, test_size=0.3, random_state=42\n",
    "    )\n",
    "    train_dataset = HydroDataset(train_graphs)\n",
    "    val_dataset = HydroDataset(val_graphs)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "config = {\n",
    "    'head_input_dim': 16,\n",
    "    'conc_input_dim': 19,\n",
    "    'hidden_dim': 96,  \n",
    "    'num_epochs': 500,\n",
    "    'lr': 1e-3,  \n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 30,\n",
    "    'save_path': './saved_models/blitz_bayesian_gnn_dual_guass',\n",
    "    'mc_samples': 10,\n",
    "    'head_prior_sigma_1': 0.01,  \n",
    "    'head_prior_sigma_2': 0.002,\n",
    "    'conc_prior_sigma_1': 0.05,  \n",
    "    'conc_prior_sigma_2': 0.002,\n",
    "    'kl_weight': 1e-4  \n",
    "}\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "\n",
    "    mask = ~np.isnan(y_true) & ~np.isinf(y_true) & ~np.isnan(y_pred) & ~np.isinf(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    r2 = sklearn_r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "def compute_uncertainty(model, data, mc_samples=10):\n",
    "    \"\"\"\n",
    "    为Blitz模型计算预测和不确定性\n",
    "    \"\"\"\n",
    "    model.train() \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(mc_samples):\n",
    "            pred = model(data)\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    predictions = torch.stack(predictions, dim=0)\n",
    "    mean_pred = predictions.mean(dim=0)\n",
    "    std_pred = predictions.std(dim=0)\n",
    "    \n",
    "    return mean_pred, std_pred\n",
    "\n",
    "def compute_feature_shap_values_improved(model, data, n_samples=20, num_samples=10):\n",
    "\n",
    "    model.train()  \n",
    "    data = data.to(next(model.parameters()).device)\n",
    "    \n",
    "    print(f\"[FeatureSHAP] 开始特征重要性分析，抽样{n_samples}个节点...\")\n",
    "    \n",
    "\n",
    "    if not hasattr(data, 'x') or not torch.is_tensor(data.x):\n",
    "        print(\"[FeatureSHAP] 错误: 数据缺少节点特征 (data.x)\")\n",
    "        return None, 0.0\n",
    "    \n",
    "    try:\n",
    "\n",
    "        current_time_step = data.time_step.unique()[0].item() if hasattr(data, 'time_step') else 0\n",
    "        time_mask = data.time_step == current_time_step if hasattr(data, 'time_step') else torch.ones(data.num_nodes, dtype=torch.bool, device=data.x.device)\n",
    "        candidate_nodes = torch.where(time_mask)[0]\n",
    "        \n",
    "        if len(candidate_nodes) == 0:\n",
    "            print(\"[FeatureSHAP] 错误: 找不到满足条件的节点\")\n",
    "            return None, 0.0\n",
    "        \n",
    "    \n",
    "        actual_n_samples = min(n_samples, len(candidate_nodes))\n",
    "        if actual_n_samples < n_samples:\n",
    "            print(f\"[FeatureSHAP] 警告: 候选节点数({len(candidate_nodes)})少于请求的样本数({n_samples})，调整为{actual_n_samples}\")\n",
    "        \n",
    "      \n",
    "        sampled_indices = torch.randperm(len(candidate_nodes))[:actual_n_samples]\n",
    "        sampled_nodes = candidate_nodes[sampled_indices]\n",
    "        \n",
    "      \n",
    "        num_features = data.x.size(1)\n",
    "        \n",
    "       \n",
    "        all_shap_values = torch.zeros(actual_n_samples, num_features, device=data.x.device)\n",
    "        all_expected_values = torch.zeros(actual_n_samples, device=data.x.device)\n",
    "        \n",
    "       \n",
    "        baseline_preds = []\n",
    "        for _ in range(num_samples):\n",
    "            with torch.no_grad():\n",
    "                pred = model(data)\n",
    "                baseline_preds.append(pred)\n",
    "        baseline_pred = torch.stack(baseline_preds, dim=0).mean(dim=0)\n",
    "        \n",
    "     \n",
    "        for i, node_idx in enumerate(sampled_nodes):\n",
    "            node_idx = node_idx.item()\n",
    "            original_value = baseline_pred[node_idx].item()\n",
    "            all_expected_values[i] = original_value\n",
    "            \n",
    " \n",
    "            for feat_idx in range(num_features):\n",
    "\n",
    "                original_feat = data.x[:, feat_idx].clone()\n",
    "                \n",
    "\n",
    "                feat_mean = original_feat.mean()\n",
    "                \n",
    "\n",
    "                data.x[:, feat_idx] = feat_mean\n",
    "                \n",
    "\n",
    "                masked_preds = []\n",
    "                for _ in range(num_samples):\n",
    "                    with torch.no_grad():\n",
    "                        masked_pred = model(data)\n",
    "                        masked_preds.append(masked_pred)\n",
    "                \n",
    "     \n",
    "                masked_pred = torch.stack(masked_preds, dim=0).mean(dim=0)\n",
    "                \n",
    "\n",
    "                shap_value = abs(original_value - masked_pred[node_idx].item())\n",
    "                all_shap_values[i, feat_idx] = shap_value\n",
    "                \n",
    "\n",
    "                data.x[:, feat_idx] = original_feat\n",
    "            \n",
    "\n",
    "            if (i + 1) % 5 == 0 or i == len(sampled_nodes) - 1:\n",
    "                print(f\"[FeatureSHAP] 已完成 {i+1}/{len(sampled_nodes)} 个节点的分析\")\n",
    "        \n",
    "\n",
    "        avg_shap_values = all_shap_values.mean(dim=0)\n",
    "        avg_expected_value = all_expected_values.mean().item()\n",
    "        \n",
    "\n",
    "        if avg_shap_values.sum() > 0:\n",
    "            avg_shap_values = avg_shap_values / avg_shap_values.sum()\n",
    "        \n",
    "        print(\"[FeatureSHAP] 特征重要性分析完成\")\n",
    "        return avg_shap_values.cpu().numpy(), avg_expected_value\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[FeatureSHAP] 特征重要性分析出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, 0.0\n",
    "\n",
    "class BlitzHeadLossAdapter(nn.Module):\n",
    "    def __init__(self, base_criterion, data, kl_weight=1e-4):\n",
    "        super().__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        self.data = data  \n",
    "        self.kl_weight = kl_weight\n",
    "    \n",
    "    def forward(self, pred, labels):\n",
    "\n",
    "        loss, _ = self.base_criterion(pred, self.data)\n",
    "        return loss\n",
    "\n",
    "class BlitzConcLossAdapter(nn.Module):\n",
    "    def __init__(self, base_criterion, data, kl_weight=5e-5):\n",
    "        super().__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        self.data = data  \n",
    "        self.kl_weight = kl_weight\n",
    "    \n",
    "    def forward(self, pred, labels):\n",
    "\n",
    "        loss, _ = self.base_criterion(pred, self.data)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4498d-f3fe-4c23-8e15-a2e91de0f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dual_model_improved(train_loader, val_loader, evaluation_criterion='r2'):\n",
    "\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"CUDA缓存已成功清除\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"无法清除CUDA缓存: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"第一阶段：开始训练水头模型\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "   \n",
    "    head_input_dim = 16 \n",
    "\n",
    "    head_model = BlitzHeadGNN(\n",
    "        node_features=head_input_dim,\n",
    "        spatial_dim=config['hidden_dim'],\n",
    "        temporal_dim=config['hidden_dim'],\n",
    "        prior_sigma_1=config['head_prior_sigma_1'],\n",
    "        prior_sigma_2=config['head_prior_sigma_2'],\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "  \n",
    "    criterion_head = ImprovedPhysicsInformedLoss(alpha=0.1, kl_weight=config['kl_weight'])\n",
    "    \n",
    "  \n",
    "    head_params = list(head_model.parameters())\n",
    "    head_optimizer = torch.optim.AdamW(head_params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    \n",
    "\n",
    "    head_scheduler = CosineAnnealingWarmRestarts(\n",
    "        head_optimizer, T_0=20, T_mult=2, eta_min=1e-5\n",
    "    )\n",
    "    \n",
    "  \n",
    "    best_head_val_loss = float('inf')\n",
    "    best_head_r2 = float('-inf')\n",
    "    head_early_stop_counter = 0\n",
    "    head_losses = {'train': [], 'val': []}\n",
    "    \n",
    "  \n",
    "    os.makedirs(config['save_path'], exist_ok=True)\n",
    "    \n",
    "    print(\"开始训练水头模型\")\n",
    "    print(f\"水头模型参数数量: {sum(p.numel() for p in head_model.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "   \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        head_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            try:\n",
    "                \n",
    "                batch = batch.to(device)\n",
    "                if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:\n",
    "                    batch.edge_attr = batch.edge_attr.float()\n",
    "                \n",
    "           \n",
    "                head_optimizer.zero_grad()\n",
    "                \n",
    "               \n",
    "                pred_head = head_model(batch)\n",
    "                \n",
    "               \n",
    "                criterion_output = criterion_head(pred_head, batch)\n",
    "                \n",
    "              \n",
    "                if isinstance(criterion_output, tuple):\n",
    "                    head_criterion_loss = criterion_output[0]\n",
    "                    if len(criterion_output) > 1:\n",
    "                        physics_loss = criterion_output[1]\n",
    "                        if isinstance(physics_loss, tuple):\n",
    "                            physics_loss_value = sum([p.item() if hasattr(p, 'item') else p for p in physics_loss])\n",
    "                        else:\n",
    "                            physics_loss_value = physics_loss.item() if hasattr(physics_loss, 'item') else physics_loss\n",
    "                    else:\n",
    "                        physics_loss_value = 0.0\n",
    "                else:\n",
    "                    head_criterion_loss = criterion_output\n",
    "                    physics_loss_value = 0.0\n",
    "                \n",
    "                kl_loss = head_model.nn_kl_divergence() * config['kl_weight']\n",
    "                total_loss = head_criterion_loss + kl_loss\n",
    "                \n",
    "               \n",
    "                total_loss.backward()\n",
    "                \n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(head_params, max_norm=1.0)\n",
    "                \n",
    "               \n",
    "                head_optimizer.step()\n",
    "                \n",
    "               \n",
    "                train_loss += total_loss.item()\n",
    "                train_batches += 1\n",
    "                \n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"水头模型 Epoch {epoch+1}, Batch {batch_idx}: \"\n",
    "                          f\"Total Loss: {total_loss.item():.4f}, \"\n",
    "                          f\"Criterion Loss: {head_criterion_loss.item():.4f}, \"\n",
    "                          f\"Physics Loss: {physics_loss_value:.4f}, \"\n",
    "                          f\"KL Loss: {kl_loss.item():.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"水头模型训练批次 {batch_idx} 出错: {e}\")\n",
    "                continue\n",
    "        \n",
    "       \n",
    "        if train_batches == 0:\n",
    "            print(\"警告: 水头模型本轮训练没有成功处理任何批次，跳过本轮\")\n",
    "            continue\n",
    "            \n",
    "      \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        \n",
    "        \n",
    "        head_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_metrics = {'mse': 0.0, 'rmse': 0.0, 'mae': 0.0, 'r2': 0.0}\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                try:\n",
    "                   \n",
    "                    batch = batch.to(device)\n",
    "                    if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:\n",
    "                        batch.edge_attr = batch.edge_attr.float()\n",
    "                    \n",
    "                  \n",
    "                    head_model.train()  #\n",
    "                    pred_head, head_std = compute_uncertainty(head_model, batch, mc_samples=config['mc_samples'])\n",
    "                    \n",
    "                 \n",
    "                    criterion_output = criterion_head(pred_head, batch)\n",
    "                    \n",
    "                 \n",
    "                    if isinstance(criterion_output, tuple):\n",
    "                        head_criterion_loss = criterion_output[0]\n",
    "                    else:\n",
    "                        head_criterion_loss = criterion_output\n",
    "                    \n",
    "                   \n",
    "                    metrics = compute_metrics(batch.head_y, pred_head)\n",
    "                    \n",
    "                    for k in metrics:\n",
    "                        val_metrics[k] += metrics[k]\n",
    "                    \n",
    "                    val_loss += head_criterion_loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"水头模型验证批次 {batch_idx} 出错: {e}\")\n",
    "                    continue\n",
    "        \n",
    "      \n",
    "        if val_batches > 0:\n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            for k in val_metrics:\n",
    "                val_metrics[k] /= val_batches\n",
    "        else:\n",
    "            print(\"警告: 水头模型本轮验证没有成功处理任何批次\")\n",
    "            avg_val_loss = float('inf')\n",
    "        \n",
    "       \n",
    "        head_losses['train'].append(avg_train_loss)\n",
    "        head_losses['val'].append({\n",
    "            'loss': avg_val_loss,\n",
    "            'metrics': val_metrics\n",
    "        })\n",
    "        \n",
    "     \n",
    "        head_scheduler.step()\n",
    "        current_lr = head_scheduler.get_last_lr()[0]\n",
    "        \n",
    "      \n",
    "        print(f\"水头模型 Epoch {epoch+1:03d}/{config['num_epochs']} | \"\n",
    "              f\"训练损失: {avg_train_loss:.4f} | 验证损失: {avg_val_loss:.4f} | \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "        print(f\"水头验证指标 - MSE: {val_metrics['mse']:.4f}, \"\n",
    "              f\"RMSE: {val_metrics['rmse']:.4f}, \"\n",
    "              f\"MAE: {val_metrics['mae']:.4f}, \"\n",
    "              f\"R2: {val_metrics['r2']:.4f}\")\n",
    "        \n",
    "   \n",
    "        if avg_val_loss < best_head_val_loss:\n",
    "            best_head_val_loss = avg_val_loss\n",
    "            try:\n",
    "                torch.save({\n",
    "                    'model_state_dict': head_model.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'config': config,\n",
    "                    'criterion': 'loss'\n",
    "                }, os.path.join(config['save_path'], 'best_head_model_loss.pth'))\n",
    "                print(f\"保存基于损失的最佳水头模型，验证损失: {best_head_val_loss:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"保存水头模型失败: {e}\")\n",
    "        \n",
    " \n",
    "        if val_metrics['r2'] > best_head_r2:\n",
    "            best_head_r2 = val_metrics['r2']\n",
    "            head_early_stop_counter = 0  \n",
    "            try:\n",
    "                torch.save({\n",
    "                    'model_state_dict': head_model.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'config': config,\n",
    "                    'criterion': 'r2'\n",
    "                }, os.path.join(config['save_path'], 'best_head_model_r2.pth'))\n",
    "                print(f\"保存基于R2的最佳水头模型，R2: {best_head_r2:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"保存水头模型失败: {e}\")\n",
    "        else:\n",
    "            head_early_stop_counter += 1\n",
    "        \n",
    "\n",
    "        if head_early_stop_counter >= config['patience']:\n",
    "            print(f\"水头模型早停触发! 在第{epoch+1}个epoch停止训练\")\n",
    "            break\n",
    "        \n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n水头模型训练完成!\")\n",
    "    print(f\"基于损失的最佳验证损失: {best_head_val_loss:.4f}\")\n",
    "    print(f\"基于R2的最佳R2分数: {best_head_r2:.4f}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"第二阶段：开始训练浓度模型\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "  \n",
    "    head_model_file = f'best_head_model_{evaluation_criterion}.pth'\n",
    "    try:\n",
    "        checkpoint = torch.load(os.path.join(config['save_path'], head_model_file),weights_only=False)\n",
    "        head_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"成功加载基于{evaluation_criterion}的最佳水头模型\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载水头模型失败，使用当前模型: {e}\")\n",
    "    \n",
    "   \n",
    "    for param in head_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    head_model.eval()\n",
    "    \n",
    "   \n",
    "    conc_input_dim = 19  \n",
    "    conc_model = BlitzConcGNN(\n",
    "        node_features=conc_input_dim,  \n",
    "        spatial_dim=config['hidden_dim'],\n",
    "        temporal_dim=config['hidden_dim'],\n",
    "        prior_sigma_1=0.1,\n",
    "        prior_sigma_2=0.01,\n",
    "        dropout=0.1,\n",
    "        posterior_mu_init=0.0,\n",
    "        posterior_rho_init=-3.0\n",
    "    ).to(device)\n",
    "    \n",
    "\n",
    "    criterion_conc = ImprovedConcLoss(kl_weight=config['kl_weight'], l1_weight=1e-5)\n",
    "    \n",
    "\n",
    "    conc_params = list(conc_model.parameters())\n",
    "    conc_optimizer = torch.optim.AdamW(conc_params, lr=config['lr'] * 0.8, weight_decay=config['weight_decay'])\n",
    "    \n",
    " \n",
    "    conc_scheduler = CosineAnnealingWarmRestarts(\n",
    "        conc_optimizer, T_0=15, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    " \n",
    "    best_conc_val_loss = float('inf')\n",
    "    best_conc_r2 = float('-inf')\n",
    "    conc_early_stop_counter = 0\n",
    "    conc_losses = {'train': [], 'val': []}\n",
    "    \n",
    "    print(f\"浓度模型参数数量: {sum(p.numel() for p in conc_model.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        conc_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        # 训练阶段\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            try:\n",
    "              \n",
    "                batch = batch.to(device)\n",
    "                if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:\n",
    "                    batch.edge_attr = batch.edge_attr.float()\n",
    "                \n",
    "             \n",
    "                with torch.no_grad():\n",
    "                    pred_head = head_model(batch)\n",
    "                \n",
    "           \n",
    "                conc_optimizer.zero_grad()\n",
    "                \n",
    "             \n",
    "                pred_conc = conc_model(batch, pred_head)\n",
    "                \n",
    "            \n",
    "                criterion_output = criterion_conc(pred_conc, batch, conc_model)\n",
    "                \n",
    "           \n",
    "                if isinstance(criterion_output, tuple):\n",
    "                    conc_criterion_loss = criterion_output[0]\n",
    "                    if len(criterion_output) > 1:\n",
    "                        loss_components = criterion_output[1]\n",
    "                        if isinstance(loss_components, tuple) and len(loss_components) >= 3:\n",
    "                            mse_loss, kl_loss_val, l1_reg = loss_components[:3]\n",
    "                        else:\n",
    "                            mse_loss, kl_loss_val, l1_reg = 0.0, 0.0, 0.0\n",
    "                    else:\n",
    "                        mse_loss, kl_loss_val, l1_reg = 0.0, 0.0, 0.0\n",
    "                else:\n",
    "                    conc_criterion_loss = criterion_output\n",
    "                    mse_loss, kl_loss_val, l1_reg = 0.0, 0.0, 0.0\n",
    "                \n",
    "                kl_loss = conc_model.nn_kl_divergence() * config['kl_weight']\n",
    "                total_loss = conc_criterion_loss + kl_loss\n",
    "                \n",
    "               \n",
    "                total_loss.backward()\n",
    "                \n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(conc_params, max_norm=1.0)\n",
    "                \n",
    "               \n",
    "                conc_optimizer.step()\n",
    "                \n",
    "               \n",
    "                train_loss += total_loss.item()\n",
    "                train_batches += 1\n",
    "                \n",
    "               \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"浓度模型 Epoch {epoch+1}, Batch {batch_idx}: \"\n",
    "                          f\"Total Loss: {total_loss.item():.4f}, \"\n",
    "                          f\"Criterion Loss: {conc_criterion_loss.item():.4f}, \"\n",
    "                          f\"MSE: {mse_loss:.4f}, \"\n",
    "                          f\"KL Loss: {kl_loss.item():.4f}, \"\n",
    "                          f\"L1 Reg: {l1_reg:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"浓度模型训练批次 {batch_idx} 出错: {e}\")\n",
    "                continue\n",
    "        \n",
    "      \n",
    "        if train_batches == 0:\n",
    "            print(\"警告: 浓度模型本轮训练没有成功处理任何批次，跳过本轮\")\n",
    "            continue\n",
    "            \n",
    "      \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        \n",
    "     \n",
    "        conc_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_metrics = {'mse': 0.0, 'rmse': 0.0, 'mae': 0.0, 'r2': 0.0}\n",
    "        val_batches = 0\n",
    "        \n",
    "       \n",
    "        all_conc_predictions = []\n",
    "        all_conc_targets = []\n",
    "        all_conc_uncertainties = []\n",
    "        all_head_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                try:\n",
    "                   \n",
    "                    batch = batch.to(device)\n",
    "                    if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:\n",
    "                        batch.edge_attr = batch.edge_attr.float()\n",
    "                    \n",
    "                   \n",
    "                    pred_head = head_model(batch)\n",
    "                    \n",
    "                   \n",
    "                    conc_model.train()  \n",
    "                    pred_conc, conc_std = compute_uncertainty(conc_model, batch, mc_samples=config['mc_samples'])\n",
    "                    \n",
    "                    \n",
    "                    criterion_output = criterion_conc(pred_conc, batch, conc_model)\n",
    "                    \n",
    "                   \n",
    "                    if isinstance(criterion_output, tuple):\n",
    "                        conc_criterion_loss = criterion_output[0]\n",
    "                    else:\n",
    "                        conc_criterion_loss = criterion_output\n",
    "                    \n",
    "               \n",
    "                    metrics = compute_metrics(batch.y, pred_conc)\n",
    "                    \n",
    "                    for k in metrics:\n",
    "                        val_metrics[k] += metrics[k]\n",
    "                    \n",
    "                    val_loss += conc_criterion_loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    # 收集预测结果用于后续分析\n",
    "                    all_conc_predictions.append(pred_conc.cpu().numpy())\n",
    "                    all_conc_targets.append(batch.y.cpu().numpy())\n",
    "                    all_conc_uncertainties.append(conc_std.cpu().numpy())\n",
    "                    all_head_predictions.append(pred_head.cpu().numpy())\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"浓度模型验证批次 {batch_idx} 出错: {e}\")\n",
    "                    continue\n",
    "        \n",
    "       \n",
    "        if val_batches > 0:\n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            for k in val_metrics:\n",
    "                val_metrics[k] /= val_batches\n",
    "        else:\n",
    "            print(\"警告: 浓度模型本轮验证没有成功处理任何批次\")\n",
    "            avg_val_loss = float('inf')\n",
    "        \n",
    "       \n",
    "        conc_losses['train'].append(avg_train_loss)\n",
    "        conc_losses['val'].append({\n",
    "            'loss': avg_val_loss,\n",
    "            'metrics': val_metrics\n",
    "        })\n",
    "        \n",
    "        \n",
    "        conc_scheduler.step()\n",
    "        current_lr = conc_scheduler.get_last_lr()[0]\n",
    "        \n",
    "  \n",
    "        print(f\"浓度模型 Epoch {epoch+1:03d}/{config['num_epochs']} | \"\n",
    "              f\"训练损失: {avg_train_loss:.4f} | 验证损失: {avg_val_loss:.4f} | \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "        print(f\"浓度验证指标 - MSE: {val_metrics['mse']:.4f}, \"\n",
    "              f\"RMSE: {val_metrics['rmse']:.4f}, \"\n",
    "              f\"MAE: {val_metrics['mae']:.4f}, \"\n",
    "              f\"R2: {val_metrics['r2']:.4f}\")\n",
    "        \n",
    "      \n",
    "        if avg_val_loss < best_conc_val_loss:\n",
    "            best_conc_val_loss = avg_val_loss\n",
    "            try:\n",
    "                torch.save({\n",
    "                    'model_state_dict': conc_model.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'config': config,\n",
    "                    'criterion': 'loss'\n",
    "                }, os.path.join(config['save_path'], 'best_conc_model_loss.pth'))\n",
    "                \n",
    "            \n",
    "                if all_conc_predictions:\n",
    "                    np.save(os.path.join(config['save_path'], 'best_conc_predictions_loss.npy'), \n",
    "                           np.concatenate(all_conc_predictions, axis=0))\n",
    "                    np.save(os.path.join(config['save_path'], 'best_conc_targets_loss.npy'), \n",
    "                           np.concatenate(all_conc_targets, axis=0))\n",
    "                    np.save(os.path.join(config['save_path'], 'best_conc_uncertainties_loss.npy'), \n",
    "                           np.concatenate(all_conc_uncertainties, axis=0))\n",
    "                \n",
    "                print(f\"保存基于损失的最佳浓度模型，验证损失: {best_conc_val_loss:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"保存浓度模型失败: {e}\")\n",
    "        \n",
    "      \n",
    "        if val_metrics['r2'] > best_conc_r2:\n",
    "            best_conc_r2 = val_metrics['r2']\n",
    "            conc_early_stop_counter = 0  \n",
    "            try:\n",
    "                torch.save({\n",
    "                    'model_state_dict': conc_model.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'config': config,\n",
    "                    'criterion': 'r2'\n",
    "                }, os.path.join(config['save_path'], 'best_conc_model_r2.pth'))\n",
    "                \n",
    "           \n",
    "                if all_conc_predictions:\n",
    "                    np.save(os.path.join(config['save_path'], 'best_conc_predictions_r2.npy'), \n",
    "                           np.concatenate(all_conc_predictions, axis=0))\n",
    "                    np.save(os.path.join(config['save_path'], 'best_conc_targets_r2.npy'), \n",
    "                           np.concatenate(all_conc_targets, axis=0))\n",
    "                    np.save(os.path.join(config['save_path'], 'best_conc_uncertainties_r2.npy'), \n",
    "                           np.concatenate(all_conc_uncertainties, axis=0))\n",
    "                \n",
    "                print(f\"保存基于R2的最佳浓度模型，R2: {best_conc_r2:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"保存浓度模型失败: {e}\")\n",
    "        else:\n",
    "            conc_early_stop_counter += 1\n",
    "        \n",
    "  \n",
    "        if conc_early_stop_counter >= config['patience']:\n",
    "            print(f\"浓度模型早停触发! 在第{epoch+1}个epoch停止训练\")\n",
    "            break\n",
    "        \n",
    "     \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n浓度模型训练完成!\")\n",
    "    print(f\"基于损失的最佳验证损失: {best_conc_val_loss:.4f}\")\n",
    "    print(f\"基于R2的最佳R2分数: {best_conc_r2:.4f}\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "  \n",
    "        head_history_data = []\n",
    "        for i, (train_loss, val_data) in enumerate(zip(head_losses['train'], head_losses['val'])):\n",
    "            head_history_data.append({\n",
    "                'epoch': i + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_data['loss'],\n",
    "                'val_mse': val_data['metrics']['mse'],\n",
    "                'val_rmse': val_data['metrics']['rmse'],\n",
    "                'val_mae': val_data['metrics']['mae'],\n",
    "                'val_r2': val_data['metrics']['r2']\n",
    "            })\n",
    "        \n",
    "        if head_history_data:\n",
    "            head_history_df = pd.DataFrame(head_history_data)\n",
    "            head_history_df.to_csv(os.path.join(config['save_path'], 'head_training_history.csv'), index=False)\n",
    "        \n",
    "      \n",
    "        conc_history_data = []\n",
    "        for i, (train_loss, val_data) in enumerate(zip(conc_losses['train'], conc_losses['val'])):\n",
    "            conc_history_data.append({\n",
    "                'epoch': i + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_data['loss'],\n",
    "                'val_mse': val_data['metrics']['mse'],\n",
    "                'val_rmse': val_data['metrics']['rmse'],\n",
    "                'val_mae': val_data['metrics']['mae'],\n",
    "                'val_r2': val_data['metrics']['r2']\n",
    "            })\n",
    "        \n",
    "        if conc_history_data:\n",
    "            conc_history_df = pd.DataFrame(conc_history_data)\n",
    "            conc_history_df.to_csv(os.path.join(config['save_path'], 'conc_training_history.csv'), index=False)\n",
    "        \n",
    "       \n",
    "        if head_history_data and conc_history_data:\n",
    "            plt.figure(figsize=(20, 12))\n",
    "            \n",
    "           \n",
    "            plt.subplot(2, 4, 1)\n",
    "            plt.plot(head_history_df['epoch'], head_history_df['train_loss'], 'b-', label='Head Train Loss')\n",
    "            plt.plot(head_history_df['epoch'], head_history_df['val_loss'], 'r-', label='Head Val Loss')\n",
    "          \n",
    "            best_loss_epoch = head_history_df.loc[head_history_df['val_loss'].idxmin(), 'epoch']\n",
    "            best_loss_value = head_history_df['val_loss'].min()\n",
    "            plt.scatter(best_loss_epoch, best_loss_value, color='red', s=100, marker='*', \n",
    "                       label=f'Best Loss (E{best_loss_epoch})')\n",
    "            plt.title('Head Model: Training and Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(2, 4, 2)\n",
    "            plt.plot(head_history_df['epoch'], head_history_df['val_r2'], 'g-', label='Head R2')\n",
    "      \n",
    "            best_r2_epoch = head_history_df.loc[head_history_df['val_r2'].idxmax(), 'epoch']\n",
    "            best_r2_value = head_history_df['val_r2'].max()\n",
    "            plt.scatter(best_r2_epoch, best_r2_value, color='green', s=100, marker='*', \n",
    "                       label=f'Best R2 (E{best_r2_epoch})')\n",
    "            plt.title('Head Model: Validation R2 Score')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('R2')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(2, 4, 3)\n",
    "            plt.plot(head_history_df['epoch'], head_history_df['val_mse'], 'orange', label='Head MSE')\n",
    "            plt.title('Head Model: Validation MSE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(2, 4, 4)\n",
    "            plt.plot(head_history_df['epoch'], head_history_df['val_rmse'], 'purple', label='Head RMSE')\n",
    "            plt.title('Head Model: Validation RMSE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('RMSE')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "     \n",
    "            plt.subplot(2, 4, 5)\n",
    "            plt.plot(conc_history_df['epoch'], conc_history_df['train_loss'], 'b--', label='Conc Train Loss')\n",
    "            plt.plot(conc_history_df['epoch'], conc_history_df['val_loss'], 'r--', label='Conc Val Loss')\n",
    "        \n",
    "            best_loss_epoch = conc_history_df.loc[conc_history_df['val_loss'].idxmin(), 'epoch']\n",
    "            best_loss_value = conc_history_df['val_loss'].min()\n",
    "            plt.scatter(best_loss_epoch, best_loss_value, color='red', s=100, marker='*', \n",
    "                       label=f'Best Loss (E{best_loss_epoch})')\n",
    "            plt.title('Conc Model: Training and Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(2, 4, 6)\n",
    "            plt.plot(conc_history_df['epoch'], conc_history_df['val_r2'], 'g--', label='Conc R2')\n",
    "      \n",
    "            best_r2_epoch = conc_history_df.loc[conc_history_df['val_r2'].idxmax(), 'epoch']\n",
    "            best_r2_value = conc_history_df['val_r2'].max()\n",
    "            plt.scatter(best_r2_epoch, best_r2_value, color='green', s=100, marker='*', \n",
    "                       label=f'Best R2 (E{best_r2_epoch})')\n",
    "            plt.title('Conc Model: Validation R2 Score')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('R2')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(2, 4, 7)\n",
    "            plt.plot(conc_history_df['epoch'], conc_history_df['val_mse'], 'orange', linestyle='--', label='Conc MSE')\n",
    "            plt.title('Conc Model: Validation MSE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(2, 4, 8)\n",
    "            plt.plot(conc_history_df['epoch'], conc_history_df['val_rmse'], 'purple', linestyle='--', label='Conc RMSE')\n",
    "            plt.title('Conc Model: Validation RMSE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('RMSE')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(config['save_path'], 'dual_model_training_curves_improved.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"改进的双模型训练曲线已保存\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"保存训练历史或绘图失败: {e}\")\n",
    "    \n",
    "    # 重新启用水头模型的梯度计算（如果需要）\n",
    "    for param in head_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"双模型训练完成总结:\")\n",
    "    print(f\"水头模型 - 基于损失的最佳验证损失: {best_head_val_loss:.4f}\")\n",
    "    print(f\"水头模型 - 基于R2的最佳R2分数: {best_head_r2:.4f}\")\n",
    "    print(f\"浓度模型 - 基于损失的最佳验证损失: {best_conc_val_loss:.4f}\")\n",
    "    print(f\"浓度模型 - 基于R2的最佳R2分数: {best_conc_r2:.4f}\")\n",
    "    print(f\"评估将使用基于{evaluation_criterion}的模型\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return head_model, conc_model, {'head': head_losses, 'conc': conc_losses}\n",
    "\n",
    "def compute_uncertainty_with_func(forward_func, data, mc_samples=30):\n",
    "\n",
    "    all_preds = []\n",
    "    for _ in range(mc_samples):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = forward_func(data)\n",
    "            all_preds.append(pred)\n",
    "    \n",
    "\n",
    "    all_preds = torch.stack(all_preds, dim=0)\n",
    "    pred_mean = all_preds.mean(dim=0)\n",
    "    pred_std = all_preds.std(dim=0)\n",
    "    \n",
    "    return pred_mean, pred_std\n",
    "\n",
    "def evaluate_dual_model_improved(head_model, conc_model, val_loader, evaluation_criterion='loss'):\n",
    "\n",
    "    print(f\"开始评估双模型性能（基于{evaluation_criterion}标准）...\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "        head_model_file = f'best_head_model_{evaluation_criterion}.pth'\n",
    "        head_checkpoint = torch.load(os.path.join(config['save_path'], head_model_file), weights_only=False)\n",
    "        head_model.load_state_dict(head_checkpoint['model_state_dict'])\n",
    "        \n",
    "        conc_model_file = f'best_conc_model_{evaluation_criterion}.pth'\n",
    "        conc_checkpoint = torch.load(os.path.join(config['save_path'], conc_model_file), weights_only=False)\n",
    "        conc_model.load_state_dict(conc_checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(f\"成功加载基于{evaluation_criterion}的最佳模型权重\")\n",
    "        print(f\"水头模型来自epoch {head_checkpoint['epoch']}, 验证损失: {head_checkpoint['val_loss']:.4f}, R2: {head_checkpoint['val_metrics']['r2']:.4f}\")\n",
    "        print(f\"浓度模型来自epoch {conc_checkpoint['epoch']}, 验证损失: {conc_checkpoint['val_loss']:.4f}, R2: {conc_checkpoint['val_metrics']['r2']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"加载模型权重失败，使用当前权重: {e}\")\n",
    "    \n",
    "    # 设置模型为训练模式以进行MC dropout\n",
    "    head_model.train()  \n",
    "    conc_model.train()\n",
    "    \n",
    "\n",
    "    results_dir = os.path.join(config['save_path'], 'evaluation_results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    all_head_preds = []\n",
    "    all_head_targets = []\n",
    "    all_head_uncertainties = []\n",
    "    all_conc_preds = []\n",
    "    all_conc_targets = []\n",
    "    all_conc_uncertainties = []\n",
    "    \n",
    "\n",
    "    predictions = []\n",
    "    uncertainties = []\n",
    "    \n",
    "\n",
    "    def head_forward_func(batch):\n",
    "\n",
    "        return head_model(batch)\n",
    "    \n",
    "    def conc_forward_func(batch_with_head):\n",
    "\n",
    "        return conc_model(batch_with_head)\n",
    "    \n",
    "    print(\"开始处理验证数据...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            try:\n",
    "                batch = batch.to(device)\n",
    "                if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:\n",
    "                    batch.edge_attr = batch.edge_attr.float()\n",
    "                \n",
    "      \n",
    "                head_pred, head_std = compute_uncertainty_with_func(\n",
    "                    head_forward_func, batch, mc_samples=config.get('mc_samples', 30)\n",
    "                )\n",
    "                \n",
    "            \n",
    "                batch_conc = batch.clone()\n",
    "              \n",
    "                if hasattr(batch_conc, 'x'):\n",
    "\n",
    "                    batch_conc.x = torch.cat([batch.x, head_pred.detach()], dim=1)\n",
    "                else:\n",
    "\n",
    "                    batch_conc.x = head_pred.detach()\n",
    "\n",
    "                conc_pred, conc_std = compute_uncertainty_with_func(\n",
    "                    conc_forward_func, batch_conc, mc_samples=config.get('mc_samples', 30)\n",
    "                )\n",
    "                \n",
    "                # 收集结果\n",
    "                all_head_preds.append(head_pred.cpu().numpy())\n",
    "                all_head_targets.append(batch.head_y.cpu().numpy())\n",
    "                all_head_uncertainties.append(head_std.cpu().numpy())\n",
    "                all_conc_preds.append(conc_pred.cpu().numpy())\n",
    "                all_conc_targets.append(batch.y.cpu().numpy())\n",
    "                all_conc_uncertainties.append(conc_std.cpu().numpy())\n",
    "                \n",
    "\n",
    "                batch_predictions = {\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'pred_head': head_pred.cpu().numpy().flatten(),\n",
    "                    'true_head': batch.head_y.cpu().numpy().flatten(),\n",
    "                    'pred_conc': conc_pred.cpu().numpy().flatten(),\n",
    "                    'true_conc': batch.y.cpu().numpy().flatten()\n",
    "                }\n",
    "                \n",
    "\n",
    "                if hasattr(batch, 'row') and hasattr(batch, 'col'):\n",
    "                    batch_predictions['row'] = batch.row.cpu().numpy()\n",
    "                    batch_predictions['col'] = batch.col.cpu().numpy()\n",
    "                if hasattr(batch, 'time_step'):\n",
    "                    batch_predictions['time_step'] = batch.time_step.cpu().numpy()\n",
    "                \n",
    "                predictions.append(batch_predictions)\n",
    "                \n",
    "\n",
    "                batch_uncertainties = {\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'head_std': head_std.cpu().numpy().flatten(),\n",
    "                    'conc_std': conc_std.cpu().numpy().flatten()\n",
    "                }\n",
    "                \n",
    "\n",
    "                if hasattr(batch, 'row') and hasattr(batch, 'col'):\n",
    "                    batch_uncertainties['row'] = batch.row.cpu().numpy()\n",
    "                    batch_uncertainties['col'] = batch.col.cpu().numpy()\n",
    "                if hasattr(batch, 'time_step'):\n",
    "                    batch_uncertainties['time_step'] = batch.time_step.cpu().numpy()\n",
    "                \n",
    "                uncertainties.append(batch_uncertainties)\n",
    "                \n",
    "                # 每10个批次输出进度\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"处理批次 {batch_idx}/{len(val_loader)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"评估批次 {batch_idx} 出错: {e}\")\n",
    "                continue\n",
    "    \n",
    "\n",
    "    all_head_preds = np.concatenate(all_head_preds, axis=0)\n",
    "    all_head_targets = np.concatenate(all_head_targets, axis=0)\n",
    "    all_head_uncertainties = np.concatenate(all_head_uncertainties, axis=0)\n",
    "    all_conc_preds = np.concatenate(all_conc_preds, axis=0)\n",
    "    all_conc_targets = np.concatenate(all_conc_targets, axis=0)\n",
    "    all_conc_uncertainties = np.concatenate(all_conc_uncertainties, axis=0)\n",
    "\n",
    "    head_metrics = compute_metrics(all_head_targets, all_head_preds)\n",
    "    conc_metrics = compute_metrics(all_conc_targets, all_conc_preds)\n",
    "    \n",
    "    print(f\"\\n水头模型评估结果（基于{evaluation_criterion}）:\")\n",
    "    for metric, value in head_metrics.items():\n",
    "        print(f\"  {metric.upper()}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n浓度模型评估结果（基于{evaluation_criterion}）:\")\n",
    "    for metric, value in conc_metrics.items():\n",
    "        print(f\"  {metric.upper()}: {value:.4f}\")\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.errorbar(all_head_targets.flatten(), all_head_preds.flatten(), \n",
    "                yerr=all_head_uncertainties.flatten(), fmt='o', alpha=0.3, \n",
    "                ecolor='lightgray', elinewidth=0.5, capsize=0, markersize=2)\n",
    "    \n",
    "    min_val = min(np.min(all_head_targets), np.min(all_head_preds))\n",
    "    max_val = max(np.max(all_head_targets), np.max(all_head_preds))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('True Head Values', fontsize=14)\n",
    "    plt.ylabel('Predicted Head Values', fontsize=14)\n",
    "    plt.title(f'Head Predictions (R² = {head_metrics[\"r2\"]:.4f}, RMSE = {head_metrics[\"rmse\"]:.4f})', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f'head_predictions_{evaluation_criterion}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.errorbar(all_conc_targets.flatten(), all_conc_preds.flatten(), \n",
    "                yerr=all_conc_uncertainties.flatten(), fmt='o', alpha=0.3,\n",
    "                ecolor='lightgray', elinewidth=0.5, capsize=0, markersize=2)\n",
    "    \n",
    "    min_val = min(np.min(all_conc_targets), np.min(all_conc_preds))\n",
    "    max_val = max(np.max(all_conc_targets), np.max(all_conc_preds))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('True Concentration Values', fontsize=14)\n",
    "    plt.ylabel('Predicted Concentration Values', fontsize=14)\n",
    "    plt.title(f'Concentration Predictions (R² = {conc_metrics[\"r2\"]:.4f}, RMSE = {conc_metrics[\"rmse\"]:.4f})', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f'conc_predictions_{evaluation_criterion}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    ax1.hist(all_head_uncertainties.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax1.set_xlabel('Head Uncertainty (Std)', fontsize=12)\n",
    "    ax1.set_ylabel('Frequency', fontsize=12)\n",
    "    ax1.set_title('Head Uncertainty Distribution', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.hist(all_conc_uncertainties.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax2.set_xlabel('Concentration Uncertainty (Std)', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Concentration Uncertainty Distribution', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f'uncertainty_distributions_{evaluation_criterion}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    head_errors = np.abs(all_head_preds.flatten() - all_head_targets.flatten())\n",
    "    conc_errors = np.abs(all_conc_preds.flatten() - all_conc_targets.flatten())\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    ax1.scatter(all_head_uncertainties.flatten(), head_errors, alpha=0.5, s=10)\n",
    "    head_corr = np.corrcoef(all_head_uncertainties.flatten(), head_errors)[0, 1]\n",
    "    ax1.set_xlabel('Head Uncertainty (Std)', fontsize=12)\n",
    "    ax1.set_ylabel('Head Absolute Error', fontsize=12)\n",
    "    ax1.set_title(f'Head: Uncertainty vs Error (r={head_corr:.3f})', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.scatter(all_conc_uncertainties.flatten(), conc_errors, alpha=0.5, s=10)\n",
    "    conc_corr = np.corrcoef(all_conc_uncertainties.flatten(), conc_errors)[0, 1]\n",
    "    ax2.set_xlabel('Concentration Uncertainty (Std)', fontsize=12)\n",
    "    ax2.set_ylabel('Concentration Absolute Error', fontsize=12)\n",
    "    ax2.set_title(f'Concentration: Uncertainty vs Error (r={conc_corr:.3f})', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f'uncertainty_vs_error_{evaluation_criterion}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    evaluation_results = {\n",
    "        'criterion': evaluation_criterion,\n",
    "        'head_metrics': head_metrics,\n",
    "        'conc_metrics': conc_metrics,\n",
    "        'head_predictions': all_head_preds,\n",
    "        'head_targets': all_head_targets,\n",
    "        'head_uncertainties': all_head_uncertainties,\n",
    "        'conc_predictions': all_conc_preds,\n",
    "        'conc_targets': all_conc_targets,\n",
    "        'conc_uncertainties': all_conc_uncertainties,\n",
    "        'detailed_predictions': predictions,\n",
    "        'detailed_uncertainties': uncertainties\n",
    "    }\n",
    "    \n",
    "\n",
    "    filename = f'dual_model_evaluation_{evaluation_criterion}.npy'\n",
    "    np.save(os.path.join(config['save_path'], filename), evaluation_results)\n",
    "    print(f\"\\n评估结果已保存到: {config['save_path']}/{filename}\")\n",
    "    \n",
    "    # 保存CSV格式的预测结果\n",
    "    if predictions:\n",
    "        pred_dfs = []\n",
    "        for pred_dict in predictions:\n",
    "            pred_df = pd.DataFrame({k: v for k, v in pred_dict.items() if not k.startswith('batch')})\n",
    "            pred_dfs.append(pred_df)\n",
    "        \n",
    "        if pred_dfs:\n",
    "            predictions_df = pd.concat(pred_dfs, ignore_index=True)\n",
    "            predictions_df.to_csv(os.path.join(results_dir, f'predictions_{evaluation_criterion}.csv'), index=False)\n",
    "            print(f\"预测结果CSV已保存到: {results_dir}/predictions_{evaluation_criterion}.csv\")\n",
    "    \n",
    "    # 保存CSV格式的不确定性结果\n",
    "    if uncertainties:\n",
    "        unc_dfs = []\n",
    "        for unc_dict in uncertainties:\n",
    "            unc_df = pd.DataFrame({k: v for k, v in unc_dict.items() if not k.startswith('batch')})\n",
    "            unc_dfs.append(unc_df)\n",
    "        \n",
    "        if unc_dfs:\n",
    "            uncertainties_df = pd.concat(unc_dfs, ignore_index=True)\n",
    "            uncertainties_df.to_csv(os.path.join(results_dir, f'uncertainties_{evaluation_criterion}.csv'), index=False)\n",
    "            print(f\"不确定性结果CSV已保存到: {results_dir}/uncertainties_{evaluation_criterion}.csv\")\n",
    "    \n",
    "    # 保存整体评估指标\n",
    "    metrics_summary = {\n",
    "        'evaluation_criterion': evaluation_criterion,\n",
    "        'head_mse': head_metrics['mse'],\n",
    "        'head_rmse': head_metrics['rmse'],\n",
    "        'head_mae': head_metrics['mae'],\n",
    "        'head_r2': head_metrics['r2'],\n",
    "        'conc_mse': conc_metrics['mse'],\n",
    "        'conc_rmse': conc_metrics['rmse'],\n",
    "        'conc_mae': conc_metrics['mae'],\n",
    "        'conc_r2': conc_metrics['r2'],\n",
    "        'head_uncertainty_mean': np.mean(all_head_uncertainties),\n",
    "        'head_uncertainty_std': np.std(all_head_uncertainties),\n",
    "        'conc_uncertainty_mean': np.mean(all_conc_uncertainties),\n",
    "        'conc_uncertainty_std': np.std(all_conc_uncertainties),\n",
    "        'head_error_uncertainty_correlation': head_corr,\n",
    "        'conc_error_uncertainty_correlation': conc_corr\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame([metrics_summary])\n",
    "    metrics_df.to_csv(os.path.join(results_dir, f'evaluation_summary_{evaluation_criterion}.csv'), index=False)\n",
    "    \n",
    "    print(f\"\\n📊 评估完成!\")\n",
    "    print(f\"📈 水头模型 - R2: {head_metrics['r2']:.4f}, RMSE: {head_metrics['rmse']:.4f}\")\n",
    "    print(f\"📈 浓度模型 - R2: {conc_metrics['r2']:.4f}, RMSE: {conc_metrics['rmse']:.4f}\")\n",
    "    print(f\"📁 所有结果已保存到: {results_dir}\")\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def compare_model_criteria(head_model, conc_model, val_loader):\n",
    "\n",
    "\n",
    "    loss_results = evaluate_dual_model_improved(head_model, conc_model, val_loader, 'loss')\n",
    "    r2_results = evaluate_dual_model_improved(head_model, conc_model, val_loader, 'r2')\n",
    "    \n",
    "\n",
    "    comparison_data = []\n",
    "    \n",
    "\n",
    "    comparison_data.append({\n",
    "        'Model': 'Head',\n",
    "        'Criterion': 'Loss',\n",
    "        'MSE': loss_results['head_metrics']['mse'],\n",
    "        'RMSE': loss_results['head_metrics']['rmse'],\n",
    "        'MAE': loss_results['head_metrics']['mae'],\n",
    "        'R2': loss_results['head_metrics']['r2'],\n",
    "        'Uncertainty_Mean': np.mean(loss_results['head_uncertainties']),\n",
    "        'Error_Uncertainty_Corr': np.corrcoef(\n",
    "            loss_results['head_uncertainties'].flatten(),\n",
    "            np.abs(loss_results['head_predictions'].flatten() - loss_results['head_targets'].flatten())\n",
    "        )[0, 1]\n",
    "    })\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': 'Head',\n",
    "        'Criterion': 'R2',\n",
    "        'MSE': r2_results['head_metrics']['mse'],\n",
    "        'RMSE': r2_results['head_metrics']['rmse'],\n",
    "        'MAE': r2_results['head_metrics']['mae'],\n",
    "        'R2': r2_results['head_metrics']['r2'],\n",
    "        'Uncertainty_Mean': np.mean(r2_results['head_uncertainties']),\n",
    "        'Error_Uncertainty_Corr': np.corrcoef(\n",
    "            r2_results['head_uncertainties'].flatten(),\n",
    "            np.abs(r2_results['head_predictions'].flatten() - r2_results['head_targets'].flatten())\n",
    "        )[0, 1]\n",
    "    })\n",
    "    \n",
    "\n",
    "    comparison_data.append({\n",
    "        'Model': 'Concentration',\n",
    "        'Criterion': 'Loss',\n",
    "        'MSE': loss_results['conc_metrics']['mse'],\n",
    "        'RMSE': loss_results['conc_metrics']['rmse'],\n",
    "        'MAE': loss_results['conc_metrics']['mae'],\n",
    "        'R2': loss_results['conc_metrics']['r2'],\n",
    "        'Uncertainty_Mean': np.mean(loss_results['conc_uncertainties']),\n",
    "        'Error_Uncertainty_Corr': np.corrcoef(\n",
    "            loss_results['conc_uncertainties'].flatten(),\n",
    "            np.abs(loss_results['conc_predictions'].flatten() - loss_results['conc_targets'].flatten())\n",
    "        )[0, 1]\n",
    "    })\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': 'Concentration',\n",
    "        'Criterion': 'R2',\n",
    "        'MSE': r2_results['conc_metrics']['mse'],\n",
    "        'RMSE': r2_results['conc_metrics']['rmse'],\n",
    "        'MAE': r2_results['conc_metrics']['mae'],\n",
    "        'R2': r2_results['conc_metrics']['r2'],\n",
    "        'Uncertainty_Mean': np.mean(r2_results['conc_uncertainties']),\n",
    "        'Error_Uncertainty_Corr': np.corrcoef(\n",
    "            r2_results['conc_uncertainties'].flatten(),\n",
    "            np.abs(r2_results['conc_predictions'].flatten() - r2_results['conc_targets'].flatten())\n",
    "        )[0, 1]\n",
    "    })\n",
    "    \n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "\n",
    "    results_dir = os.path.join(config['save_path'], 'evaluation_results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    comparison_df.to_csv(os.path.join(results_dir, 'model_criteria_comparison.csv'), index=False)\n",
    "    \n",
    "\n",
    "    print(\"\\n模型选择标准比较结果:\")\n",
    "    print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # 绘制扩展的比较图\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # R2比较\n",
    "    plt.subplot(2, 4, 1)\n",
    "    head_r2 = [loss_results['head_metrics']['r2'], r2_results['head_metrics']['r2']]\n",
    "    conc_r2 = [loss_results['conc_metrics']['r2'], r2_results['conc_metrics']['r2']]\n",
    "    x = ['Loss-based', 'R2-based']\n",
    "    plt.bar([0, 1], head_r2, alpha=0.7, label='Head Model', width=0.35)\n",
    "    plt.bar([0.35, 1.35], conc_r2, alpha=0.7, label='Concentration Model', width=0.35)\n",
    "    plt.xlabel('Model Selection Criterion')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.title('R2 Score Comparison')\n",
    "    plt.xticks([0.175, 1.175], x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # MSE比较\n",
    "    plt.subplot(2, 4, 2)\n",
    "    head_mse = [loss_results['head_metrics']['mse'], r2_results['head_metrics']['mse']]\n",
    "    conc_mse = [loss_results['conc_metrics']['mse'], r2_results['conc_metrics']['mse']]\n",
    "    plt.bar([0, 1], head_mse, alpha=0.7, label='Head Model', width=0.35)\n",
    "    plt.bar([0.35, 1.35], conc_mse, alpha=0.7, label='Concentration Model', width=0.35)\n",
    "    plt.xlabel('Model Selection Criterion')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('MSE Comparison')\n",
    "    plt.xticks([0.175, 1.175], x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "\n",
    "    plt.subplot(2, 4, 3)\n",
    "    head_unc = [np.mean(loss_results['head_uncertainties']), np.mean(r2_results['head_uncertainties'])]\n",
    "    conc_unc = [np.mean(loss_results['conc_uncertainties']), np.mean(r2_results['conc_uncertainties'])]\n",
    "    plt.bar([0, 1], head_unc, alpha=0.7, label='Head Model', width=0.35)\n",
    "    plt.bar([0.35, 1.35], conc_unc, alpha=0.7, label='Concentration Model', width=0.35)\n",
    "    plt.xlabel('Model Selection Criterion')\n",
    "    plt.ylabel('Mean Uncertainty')\n",
    "    plt.title('Uncertainty Comparison')\n",
    "    plt.xticks([0.175, 1.175], x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "\n",
    "    plt.subplot(2, 4, 4)\n",
    "    head_corr = [comparison_data[0]['Error_Uncertainty_Corr'], comparison_data[1]['Error_Uncertainty_Corr']]\n",
    "    conc_corr = [comparison_data[2]['Error_Uncertainty_Corr'], comparison_data[3]['Error_Uncertainty_Corr']]\n",
    "    plt.bar([0, 1], head_corr, alpha=0.7, label='Head Model', width=0.35)\n",
    "    plt.bar([0.35, 1.35], conc_corr, alpha=0.7, label='Concentration Model', width=0.35)\n",
    "    plt.xlabel('Model Selection Criterion')\n",
    "    plt.ylabel('Error-Uncertainty Correlation')\n",
    "    plt.title('Calibration Quality Comparison')\n",
    "    plt.xticks([0.175, 1.175], x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "\n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.scatter(loss_results['head_targets'].flatten(), loss_results['head_predictions'].flatten(), \n",
    "               alpha=0.5, label='Loss-based', s=5)\n",
    "    plt.scatter(r2_results['head_targets'].flatten(), r2_results['head_predictions'].flatten(), \n",
    "               alpha=0.5, label='R2-based', s=5)\n",
    "    plt.plot([loss_results['head_targets'].min(), loss_results['head_targets'].max()], \n",
    "             [loss_results['head_targets'].min(), loss_results['head_targets'].max()], 'r--', alpha=0.8)\n",
    "    plt.xlabel('True Head Values')\n",
    "    plt.ylabel('Predicted Head Values')\n",
    "    plt.title('Head Model: True vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.scatter(loss_results['conc_targets'].flatten(), loss_results['conc_predictions'].flatten(), \n",
    "               alpha=0.5, label='Loss-based', s=5)\n",
    "    plt.scatter(r2_results['conc_targets'].flatten(), r2_results['conc_predictions'].flatten(), \n",
    "               alpha=0.5, label='R2-based', s=5)\n",
    "    plt.plot([loss_results['conc_targets'].min(), loss_results['conc_targets'].max()], \n",
    "             [loss_results['conc_targets'].min(), loss_results['conc_targets'].max()], 'r--', alpha=0.8)\n",
    "    plt.xlabel('True Concentration Values')\n",
    "    plt.ylabel('Predicted Concentration Values')\n",
    "    plt.title('Concentration Model: True vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "\n",
    "    plt.subplot(2, 4, 7)\n",
    "    plt.hist(loss_results['head_uncertainties'].flatten(), bins=30, alpha=0.5, label='Loss-based', density=True)\n",
    "    plt.hist(r2_results['head_uncertainties'].flatten(), bins=30, alpha=0.5, label='R2-based', density=True)\n",
    "    plt.xlabel('Head Uncertainty')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Head Uncertainty Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.hist(loss_results['conc_uncertainties'].flatten(), bins=30, alpha=0.5, label='Loss-based', density=True)\n",
    "    plt.hist(r2_results['conc_uncertainties'].flatten(), bins=30, alpha=0.5, label='R2-based', density=True)\n",
    "    plt.xlabel('Concentration Uncertainty')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Concentration Uncertainty Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'model_criteria_comparison_comprehensive.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n比较图表已保存到: {results_dir}/model_criteria_comparison_comprehensive.png\")\n",
    "    print(f\"比较数据已保存到: {results_dir}/model_criteria_comparison.csv\")\n",
    "    \n",
    "    return comparison_df, loss_results, r2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4f1c9-8e2b-4b4c-a395-8432de93e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('conc_dual_guass.csv')  \n",
    "train_loader, val_loader = prepare_data(data, batch_size=4)\n",
    "head_model, conc_model, training_losses = train_dual_model_improved(train_loader, val_loader)\n",
    "evaluation_results = evaluate_dual_model_improved(head_model, conc_model, val_loader)\n",
    "comparison_df, loss_results, r2_results=compare_model_criteria(head_model, conc_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538157f9-fd49-4bec-b32b-a2f18599af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_complete_data_copy(original_data, device):\n",
    "    \"\"\"创建包含所有必要属性的完整数据副本\"\"\"\n",
    "    try:\n",
    "        data_copy = original_data.clone().to(device)\n",
    "        \n",
    "        required_attrs = ['x', 'conc_x', 'edge_index', 'edge_attr', 'y', 'head_y', \n",
    "                         'time_step', 'bc_mask', 'row', 'col', 'model_name']\n",
    "        \n",
    "        for attr in required_attrs:\n",
    "            if hasattr(original_data, attr):\n",
    "                original_value = getattr(original_data, attr)\n",
    "                if original_value is not None:\n",
    "                    if torch.is_tensor(original_value):\n",
    "                        setattr(data_copy, attr, original_value.clone().to(device))\n",
    "                    else:\n",
    "                        setattr(data_copy, attr, original_value)\n",
    "            else:\n",
    "                if attr == 'bc_mask':\n",
    "                    if hasattr(data_copy, 'x') and data_copy.x is not None:\n",
    "                        num_nodes = data_copy.x.shape[0]\n",
    "                        setattr(data_copy, attr, torch.zeros(num_nodes, 5, dtype=torch.float32, device=device))\n",
    "                elif attr == 'time_step':\n",
    "                    if hasattr(data_copy, 'x') and data_copy.x is not None:\n",
    "                        num_nodes = data_copy.x.shape[0]\n",
    "                        setattr(data_copy, attr, torch.zeros(num_nodes, dtype=torch.long, device=device))\n",
    "        \n",
    "        return data_copy\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 创建数据副本失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def compute_feature_effects_and_importance(model, data, model_type='head', mc_samples=20, perturbation_scale=0.1):\n",
    "    \"\"\"\n",
    "    计算特征对模型输出的作用（正向/负向）和重要性\n",
    "    \n",
    "    Args:\n",
    "        model: 要分析的模型\n",
    "        data: 输入数据\n",
    "        model_type: 模型类型 ('head' 或 'conc')\n",
    "        mc_samples: MC采样次数\n",
    "        perturbation_scale: 扰动强度\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含特征作用和重要性的字典\n",
    "    \"\"\"\n",
    "    model.train()  \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    print(f\"  🔍 计算{model_type.upper()}模型的特征作用和重要性...\")\n",
    "    \n",
    "    try:\n",
    "       \n",
    "        data_copy = create_complete_data_copy(data, device)\n",
    "        if data_copy is None:\n",
    "            return {}\n",
    "        \n",
    "        # 确定特征集\n",
    "        if model_type == 'head':\n",
    "            if not hasattr(data_copy, 'x') or data_copy.x is None:\n",
    "                print(f\"    ❌ 水头模型缺少节点特征 x\")\n",
    "                return {}\n",
    "            feature_tensor = data_copy.x\n",
    "            feature_name = 'x'\n",
    "        else:  # conc\n",
    "            if not hasattr(data_copy, 'conc_x') or data_copy.conc_x is None:\n",
    "                print(f\"    ❌ 浓度模型缺少节点特征 conc_x\")\n",
    "                return {}\n",
    "            feature_tensor = data_copy.conc_x\n",
    "            feature_name = 'conc_x'\n",
    "        \n",
    "        num_features = feature_tensor.shape[1]\n",
    "        print(f\"    📊 特征数量: {num_features}\")\n",
    "        \n",
    "        # 获取基线预测\n",
    "        baseline_preds = []\n",
    "        for _ in range(mc_samples):\n",
    "            with torch.no_grad():\n",
    "                pred = model(data_copy)\n",
    "                baseline_preds.append(pred.detach())\n",
    "        \n",
    "        baseline_pred = torch.stack(baseline_preds, dim=0).mean(dim=0)\n",
    "        baseline_mean = baseline_pred.mean().item()\n",
    "        \n",
    "        print(f\"    📈 基线预测均值: {baseline_mean:.6f}\")\n",
    "        \n",
    "        # 存储每个特征的作用和重要性\n",
    "        feature_effects = np.zeros(num_features)  # 特征作用（正向/负向）\n",
    "        feature_importance = np.zeros(num_features)  # 特征重要性（绝对值）\n",
    "        \n",
    "        # 对每个特征进行扰动分析\n",
    "        for feat_idx in range(num_features):\n",
    "            print(f\"      分析特征 {feat_idx + 1}/{num_features}\")\n",
    "            \n",
    "            # 正向扰动\n",
    "            data_pos = data_copy.clone()\n",
    "            original_feature = getattr(data_pos, feature_name)[:, feat_idx].clone()\n",
    "            std_val = original_feature.std().item()\n",
    "            \n",
    "            # 正向扰动\n",
    "            getattr(data_pos, feature_name)[:, feat_idx] = original_feature + perturbation_scale * std_val\n",
    "            \n",
    "            pos_preds = []\n",
    "            for _ in range(mc_samples):\n",
    "                with torch.no_grad():\n",
    "                    pred = model(data_pos)\n",
    "                    pos_preds.append(pred.detach())\n",
    "            pos_pred = torch.stack(pos_preds, dim=0).mean(dim=0)\n",
    "            pos_effect = pos_pred.mean().item() - baseline_mean\n",
    "            \n",
    "   \n",
    "            data_neg = data_copy.clone()\n",
    "            getattr(data_neg, feature_name)[:, feat_idx] = original_feature - perturbation_scale * std_val\n",
    "            \n",
    "            neg_preds = []\n",
    "            for _ in range(mc_samples):\n",
    "                with torch.no_grad():\n",
    "                    pred = model(data_neg)\n",
    "                    neg_preds.append(pred.detach())\n",
    "            neg_pred = torch.stack(neg_preds, dim=0).mean(dim=0)\n",
    "            neg_effect = neg_pred.mean().item() - baseline_mean\n",
    "            \n",
    "           \n",
    "            avg_effect = (pos_effect - neg_effect) / 2\n",
    "            \n",
    "          \n",
    "            importance = abs(pos_effect) + abs(neg_effect)\n",
    "            \n",
    "            feature_effects[feat_idx] = avg_effect\n",
    "            feature_importance[feat_idx] = importance\n",
    "        \n",
    "  \n",
    "        if feature_importance.max() > 0:\n",
    "            feature_importance = feature_importance / feature_importance.max()\n",
    "        \n",
    "        print(f\"    ✅ 特征作用和重要性计算完成\")\n",
    "        \n",
    "        return {\n",
    "            'effects': feature_effects,\n",
    "            'importance': feature_importance,\n",
    "            'baseline': baseline_mean\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ❌ 特征作用计算失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "def prepare_single_graph_batch(batch):\n",
    "    \"\"\"从batch中提取单个图的数据\"\"\"\n",
    "    try:\n",
    "        if not hasattr(batch, 'batch') or batch.batch is None:\n",
    "            return batch\n",
    "        \n",
    "        unique_graphs = torch.unique(batch.batch)\n",
    "        if len(unique_graphs) <= 1:\n",
    "            return batch\n",
    "        \n",
    " \n",
    "        first_graph_mask = batch.batch == unique_graphs[0]\n",
    "        new_batch = type(batch)()\n",
    "        \n",
    "\n",
    "        node_attrs = ['x', 'conc_x', 'head_y', 'y', 'time_step', 'bc_mask', 'row', 'col']\n",
    "        for attr in node_attrs:\n",
    "            if hasattr(batch, attr) and getattr(batch, attr) is not None:\n",
    "                attr_value = getattr(batch, attr)\n",
    "                if torch.is_tensor(attr_value) and len(attr_value) == len(first_graph_mask):\n",
    "                    setattr(new_batch, attr, attr_value[first_graph_mask])\n",
    "        \n",
    "\n",
    "        if hasattr(batch, 'edge_index') and batch.edge_index is not None:\n",
    "            first_nodes = torch.where(first_graph_mask)[0]\n",
    "            edge_mask = torch.isin(batch.edge_index[0], first_nodes) & \\\n",
    "                       torch.isin(batch.edge_index[1], first_nodes)\n",
    "            \n",
    "            if edge_mask.sum() > 0:\n",
    "                node_mapping = {old.item(): new for new, old in enumerate(first_nodes)}\n",
    "                old_edges = batch.edge_index[:, edge_mask]\n",
    "                new_edges = torch.zeros_like(old_edges)\n",
    "                \n",
    "                for i in range(old_edges.shape[1]):\n",
    "                    new_edges[0, i] = node_mapping[old_edges[0, i].item()]\n",
    "                    new_edges[1, i] = node_mapping[old_edges[1, i].item()]\n",
    "                \n",
    "                new_batch.edge_index = new_edges\n",
    "                \n",
    "                if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:\n",
    "                    new_batch.edge_attr = batch.edge_attr[edge_mask]\n",
    "            else:\n",
    "                new_batch.edge_index = torch.empty((2, 0), dtype=torch.long, device=batch.edge_index.device)\n",
    "        \n",
    "\n",
    "        other_attrs = ['model_name', 'time_steps']\n",
    "        for attr in other_attrs:\n",
    "            if hasattr(batch, attr):\n",
    "                setattr(new_batch, attr, getattr(batch, attr))\n",
    "        \n",
    "        return new_batch\n",
    "    except Exception as e:\n",
    "        print(f\"准备单图batch失败: {e}\")\n",
    "        return batch\n",
    "\n",
    "def analyze_model_effects(model, val_loader, config, model_type='head', \n",
    "                         evaluation_criterion='loss', n_samples=300):\n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "        model_file = f'best_{model_type}_model_{evaluation_criterion}.pth'\n",
    "        checkpoint = torch.load(os.path.join(config['save_path'], model_file), \n",
    "                              map_location='cpu', weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"✅ 成功加载{model_type.upper()}模型 (Epoch: {checkpoint['epoch']}, Val Loss: {checkpoint['val_loss']:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  加载{model_type.upper()}模型权重失败: {e}\")\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    if model_type == 'head':\n",
    "        feature_names = [\n",
    "            'x', 'y', 'top', 'bottom', 'K', 'recharge', 'ET',\n",
    "            'river_stage', 'river_cond', 'river_rbot', 'well_rate', \n",
    "            'well_mask', 'chd_mask', 'lytyp', 'prev_head', 'prev2_head'\n",
    "        ]\n",
    "    else:  \n",
    "        feature_names = [\n",
    "            'x', 'y', 'top', 'bottom', 'K', 'recharge', 'ET',\n",
    "            'river_stage', 'river_cond', 'river_rbot', 'well_rate', \n",
    "            'well_mask', 'chd_mask', 'lytyp', 'conc_mask',\n",
    "            'prev_head', 'prev2_head', 'prev_conc', 'prev2_conc'\n",
    "        ]\n",
    "    \n",
    "   \n",
    "    all_effect_data = []\n",
    "    successful_samples = 0\n",
    "    total_batches = len(val_loader)\n",
    "    \n",
    "    print(f\"🎯 开始分析 {n_samples} 个样本...\")\n",
    "    print(f\"📊 验证数据加载器总批次数: {total_batches}\")\n",
    "    \n",
    " \n",
    "    cycles_needed = max(1, (n_samples + total_batches - 1) // total_batches)\n",
    "    print(f\"🔄 需要重复遍历数据加载器 {cycles_needed} 次\")\n",
    "    \n",
    "    for cycle in range(cycles_needed):\n",
    "        print(f\"\\n🔄 第 {cycle + 1}/{cycles_needed} 轮遍历...\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            if successful_samples >= n_samples:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "               \n",
    "                single_batch = prepare_single_graph_batch(batch)\n",
    "                \n",
    "        \n",
    "                result = compute_feature_effects_and_importance(\n",
    "                    model, single_batch, model_type=model_type\n",
    "                )\n",
    "                \n",
    "                if result and 'effects' in result and 'importance' in result:\n",
    "                    effects = result['effects']\n",
    "                    importance = result['importance']\n",
    "                    baseline = result['baseline']\n",
    "                    \n",
    "           \n",
    "                    for i, (effect, imp) in enumerate(zip(effects, importance)):\n",
    "                        feature_name = feature_names[i] if i < len(feature_names) else f'Feature_{i}'\n",
    "                        all_effect_data.append({\n",
    "                            'Sample': successful_samples + 1,\n",
    "                            'Feature': feature_name,\n",
    "                            'Effect': effect,  \n",
    "                            'Importance': imp,  \n",
    "                            'Feature_Index': i,\n",
    "                            'Baseline': baseline,\n",
    "                            'Cycle': cycle + 1,\n",
    "                            'Batch_in_Cycle': batch_idx + 1\n",
    "                        })\n",
    "                    \n",
    "                    successful_samples += 1\n",
    "                    print(f\"  ✅ 样本 {successful_samples}/{n_samples} 分析完成 (轮次{cycle+1}, 批次{batch_idx+1})\")\n",
    "                else:\n",
    "                    print(f\"  ❌ 样本分析失败 (轮次{cycle+1}, 批次{batch_idx+1})\")\n",
    "                \n",
    "     \n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ 处理批次 {batch_idx} 失败: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if successful_samples >= n_samples:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n📊 {model_type.upper()}模型分析完成，成功分析 {successful_samples} 个样本\")\n",
    "    \n",
    "    if not all_effect_data:\n",
    "        print(\"❌ 没有收集到任何有效数据\")\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "    effect_df = pd.DataFrame(all_effect_data)\n",
    "    \n",
    "\n",
    "    feature_stats = effect_df.groupby('Feature').agg({\n",
    "        'Effect': ['mean', 'std', 'min', 'max'],\n",
    "        'Importance': ['mean', 'std', 'min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "\n",
    "    feature_stats.columns = ['Feature', 'Effect_Mean', 'Effect_Std', 'Effect_Min', 'Effect_Max',\n",
    "                            'Importance_Mean', 'Importance_Std', 'Importance_Min', 'Importance_Max']\n",
    "    \n",
    "    # 按重要性排序\n",
    "    feature_stats = feature_stats.sort_values('Importance_Mean', ascending=False)\n",
    "    \n",
    "    return effect_df, feature_stats\n",
    "\n",
    "def create_effect_swarm_plots(head_df, head_stats, conc_df, conc_stats, config, \n",
    "                             evaluation_criterion):\n",
    "    \"\"\"\n",
    "    创建基于特征作用的蜂群图（横轴是作用，颜色是重要性）\n",
    "    特征重要性从上到下排序\n",
    "    \"\"\"\n",
    "    print(\"🎨 生成特征作用蜂群图...\")\n",
    "\n",
    "    # 创建结果目录\n",
    "    results_dir = os.path.join(config['save_path'], 'effect_swarm_analysis')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "\n",
    "    if head_df is not None:\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "   \n",
    "        feature_order = head_stats.sort_values(\"Importance_Mean\", ascending=False)['Feature'].tolist()\n",
    "        \n",
    "\n",
    "        y_mapping = {f: i for i, f in enumerate(feature_order)}\n",
    "\n",
    "\n",
    "        scatter = plt.scatter(\n",
    "            head_df['Effect'], \n",
    "            head_df['Feature'].map(y_mapping), \n",
    "            c=head_df['Importance'], \n",
    "            cmap='viridis', \n",
    "            s=80, \n",
    "            alpha=0.7,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        \n",
    "\n",
    "        cbar = plt.colorbar(scatter)\n",
    "        cbar.set_label('Feature Importance', fontsize=12, fontweight='bold')\n",
    "\n",
    "\n",
    "        plt.axvline(x=0, color='red', linestyle='--', alpha=0.8, linewidth=2, label='No Effect')\n",
    "\n",
    "\n",
    "        for i, feature in enumerate(feature_order):\n",
    "            mean_effect = head_stats[head_stats['Feature'] == feature]['Effect_Mean'].iloc[0]\n",
    "            plt.scatter(mean_effect, i, marker='D', s=100, color='red', \n",
    "                       edgecolors='black', linewidth=1, zorder=5)\n",
    "\n",
    "            plt.text(mean_effect, i + 0.15, f'{mean_effect:.4f}', \n",
    "                    ha='center', va='bottom', fontsize=9, fontweight='bold', \n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "        plt.yticks(range(len(feature_order)), feature_order)\n",
    "        plt.xlabel('Feature Effect on Model Output', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Features (sorted by importance)', fontsize=14, fontweight='bold')\n",
    "        plt.title(f'Head Model: Feature Effects Distribution\\nCriterion: {evaluation_criterion}\\n(Red diamonds = mean effect)', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, f'head_effect_swarm_{evaluation_criterion}_sorted.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"  📊 Head模型作用蜂群图已保存（已按重要性排序）\")\n",
    "\n",
    "    if conc_df is not None:\n",
    "        plt.figure(figsize=(16, 14))\n",
    "        \n",
    "        # 按重要性均值从高到低排序\n",
    "        feature_order = conc_stats.sort_values(\"Importance_Mean\", ascending=False)['Feature'].tolist()\n",
    "        y_mapping = {f: i for i, f in enumerate(feature_order)}\n",
    "\n",
    "        scatter = plt.scatter(\n",
    "            conc_df['Effect'], \n",
    "            conc_df['Feature'].map(y_mapping), \n",
    "            c=conc_df['Importance'], \n",
    "            cmap='viridis', \n",
    "            s=80, \n",
    "            alpha=0.7,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "\n",
    "        cbar = plt.colorbar(scatter)\n",
    "        cbar.set_label('Feature Importance', fontsize=12, fontweight='bold')\n",
    "\n",
    "        plt.axvline(x=0, color='red', linestyle='--', alpha=0.8, linewidth=2, label='No Effect')\n",
    "\n",
    "        for i, feature in enumerate(feature_order):\n",
    "            mean_effect = conc_stats[conc_stats['Feature'] == feature]['Effect_Mean'].iloc[0]\n",
    "            plt.scatter(mean_effect, i, marker='D', s=100, color='red', \n",
    "                       edgecolors='black', linewidth=1, zorder=5)\n",
    "            plt.text(mean_effect, i + 0.15, f'{mean_effect:.4f}', \n",
    "                    ha='center', va='bottom', fontsize=9, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "        plt.yticks(range(len(feature_order)), feature_order)\n",
    "        plt.xlabel('Feature Effect on Model Output', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Features (sorted by importance)', fontsize=14, fontweight='bold')\n",
    "        plt.title(f'Concentration Model: Feature Effects Distribution\\nCriterion: {evaluation_criterion}\\n(Red diamonds = mean effect)', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, f'conc_effect_swarm_{evaluation_criterion}_sorted.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"  📊 Conc模型作用蜂群图已保存（已按重要性排序）\")\n",
    "\n",
    "\n",
    "    if head_df is not None and conc_df is not None:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(28, 14))\n",
    "\n",
    "\n",
    "        top_head_features = head_stats.sort_values(\"Importance_Mean\", ascending=False).head(10)['Feature'].tolist()\n",
    "        head_top_df = head_df[head_df['Feature'].isin(top_head_features)]\n",
    "        y_mapping_head = {f: i for i, f in enumerate(top_head_features)}\n",
    "\n",
    "        ax1.scatter(\n",
    "            head_top_df['Effect'], \n",
    "            head_top_df['Feature'].map(y_mapping_head), \n",
    "            c=head_top_df['Importance'], \n",
    "            cmap='viridis', \n",
    "            s=100, \n",
    "            alpha=0.8,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        ax1.axvline(x=0, color='red', linestyle='--', alpha=0.8, linewidth=2)\n",
    "        for i, feature in enumerate(top_head_features):\n",
    "            mean_effect = head_stats[head_stats['Feature'] == feature]['Effect_Mean'].iloc[0]\n",
    "            ax1.scatter(mean_effect, i, marker='D', s=120, color='red', \n",
    "                       edgecolors='black', linewidth=1, zorder=5)\n",
    "            ax1.text(mean_effect, i + 0.15, f'{mean_effect:.4f}', \n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "        ax1.set_yticks(range(len(top_head_features)))\n",
    "        ax1.set_yticklabels(top_head_features)\n",
    "        ax1.set_xlabel('Feature Effect on Output', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Features (sorted by importance)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('Head Model - Top 10 Features', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "\n",
    "        top_conc_features = conc_stats.sort_values(\"Importance_Mean\", ascending=False).head(10)['Feature'].tolist()\n",
    "        conc_top_df = conc_df[conc_df['Feature'].isin(top_conc_features)]\n",
    "        y_mapping_conc = {f: i for i, f in enumerate(top_conc_features)}\n",
    "\n",
    "        ax2.scatter(\n",
    "            conc_top_df['Effect'], \n",
    "            conc_top_df['Feature'].map(y_mapping_conc), \n",
    "            c=conc_top_df['Importance'], \n",
    "            cmap='viridis', \n",
    "            s=100, \n",
    "            alpha=0.8,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.8, linewidth=2)\n",
    "        for i, feature in enumerate(top_conc_features):\n",
    "            mean_effect = conc_stats[conc_stats['Feature'] == feature]['Effect_Mean'].iloc[0]\n",
    "            ax2.scatter(mean_effect, i, marker='D', s=120, color='red', \n",
    "                       edgecolors='black', linewidth=1, zorder=5)\n",
    "            ax2.text(mean_effect, i + 0.15, f'{mean_effect:.4f}', \n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "        ax2.set_yticks(range(len(top_conc_features)))\n",
    "        ax2.set_yticklabels(top_conc_features)\n",
    "        ax2.set_xlabel('Feature Effect on Output', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('Features (sorted by importance)', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Concentration Model - Top 10 Features', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    " \n",
    "        fig.subplots_adjust(right=0.85)\n",
    "        cbar_ax = fig.add_axes([0.87, 0.15, 0.02, 0.7])\n",
    "        cbar = fig.colorbar(ax2.collections[0], cax=cbar_ax)\n",
    "        cbar.set_label('Feature Importance', fontsize=12, fontweight='bold')\n",
    "\n",
    "        plt.suptitle(f'Model Comparison: Feature Effects on Output\\nCriterion: {evaluation_criterion}', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.savefig(os.path.join(results_dir, f'comparison_effect_swarm_{evaluation_criterion}_sorted.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"  📊 对比作用蜂群图已保存（已按重要性排序）\")\n",
    "\n",
    "    return results_dir\n",
    "\n",
    "def save_effect_results(head_df, head_stats, conc_df, conc_stats, results_dir, \n",
    "                       evaluation_criterion):\n",
    "    \"\"\"\n",
    "    保存特征作用分析结果\n",
    "    \"\"\"\n",
    "    print(\"💾 保存特征作用分析结果...\")\n",
    "    \n",
    "\n",
    "    if head_df is not None:\n",
    "        head_df.to_csv(os.path.join(results_dir, f'head_effects_raw_{evaluation_criterion}.csv'), \n",
    "                      index=False)\n",
    "        head_stats.to_csv(os.path.join(results_dir, f'head_effects_stats_{evaluation_criterion}.csv'), \n",
    "                         index=False)\n",
    "    \n",
    "    if conc_df is not None:\n",
    "        conc_df.to_csv(os.path.join(results_dir, f'conc_effects_raw_{evaluation_criterion}.csv'), \n",
    "                      index=False)\n",
    "        conc_stats.to_csv(os.path.join(results_dir, f'conc_effects_stats_{evaluation_criterion}.csv'), \n",
    "                         index=False)\n",
    "    \n",
    "\n",
    "    summary_data = []\n",
    "    \n",
    "    if head_stats is not None:\n",
    "        for _, row in head_stats.iterrows():\n",
    "            summary_data.append({\n",
    "                'Model': 'Head',\n",
    "                'Feature': row['Feature'],\n",
    "                'Mean_Effect': row['Effect_Mean'],\n",
    "                'Effect_Direction': 'Positive' if row['Effect_Mean'] > 0 else 'Negative' if row['Effect_Mean'] < 0 else 'Neutral',\n",
    "                'Mean_Importance': row['Importance_Mean'],\n",
    "                'Effect_Std': row['Effect_Std'],\n",
    "                'Importance_Std': row['Importance_Std']\n",
    "            })\n",
    "    \n",
    "    if conc_stats is not None:\n",
    "        for _, row in conc_stats.iterrows():\n",
    "            summary_data.append({\n",
    "                'Model': 'Concentration',\n",
    "                'Feature': row['Feature'],\n",
    "                'Mean_Effect': row['Effect_Mean'],\n",
    "                'Effect_Direction': 'Positive' if row['Effect_Mean'] > 0 else 'Negative' if row['Effect_Mean'] < 0 else 'Neutral',\n",
    "                'Mean_Importance': row['Importance_Mean'],\n",
    "                'Effect_Std': row['Effect_Std'],\n",
    "                'Importance_Std': row['Importance_Std']\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(os.path.join(results_dir, f'feature_effects_summary_{evaluation_criterion}.csv'), \n",
    "                         index=False)\n",
    "        \n",
    "        print(f\"  📄 总结报告已保存\")\n",
    "\n",
    "def run_effect_swarm_analysis(head_model, conc_model, val_loader, config, \n",
    "                             evaluation_criterion='loss', n_samples=300):\n",
    "    \"\"\"\n",
    "    运行基于特征作用的蜂群图分析\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🎯 特征作用蜂群图分析\")\n",
    "    print(\"  横轴：特征对模型输出的作用（正向/负向）\")\n",
    "    print(\"  颜色：特征重要性\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "\n",
    "    print(\"\\n🌊 分析Head模型特征作用...\")\n",
    "    head_df, head_stats = analyze_model_effects(\n",
    "        head_model, val_loader, config, model_type='head', \n",
    "        evaluation_criterion=evaluation_criterion, n_samples=n_samples\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(\"\\n🧪 分析Conc模型特征作用...\")\n",
    "    conc_df, conc_stats = analyze_model_effects(\n",
    "        conc_model, val_loader, config, model_type='conc', \n",
    "        evaluation_criterion=evaluation_criterion, n_samples=n_samples\n",
    "    )\n",
    "    \n",
    "\n",
    "    if head_df is not None or conc_df is not None:\n",
    "        results_dir = create_effect_swarm_plots(\n",
    "            head_df, head_stats, conc_df, conc_stats, \n",
    "            config, evaluation_criterion\n",
    "        )\n",
    "        \n",
    "\n",
    "        save_effect_results(\n",
    "            head_df, head_stats, conc_df, conc_stats, \n",
    "            results_dir, evaluation_criterion\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 特征作用蜂群图分析完成!\")\n",
    "        print(f\"📁 结果保存至: {results_dir}\")\n",
    "        \n",
    "   \n",
    "        if head_stats is not None:\n",
    "            print(f\"\\n🏆 Head模型 Top 5 重要特征作用:\")\n",
    "            for i, row in head_stats.head(5).iterrows():\n",
    "                direction = \"正向\" if row['Effect_Mean'] > 0 else \"负向\" if row['Effect_Mean'] < 0 else \"中性\"\n",
    "                print(f\"  {i+1}. {row['Feature']}: 作用 {row['Effect_Mean']:.6f} ({direction}), 重要性 {row['Importance_Mean']:.6f}\")\n",
    "        \n",
    "        if conc_stats is not None:\n",
    "            print(f\"\\n🏆 Conc模型 Top 5 重要特征作用:\")\n",
    "            for i, row in conc_stats.head(5).iterrows():\n",
    "                direction = \"正向\" if row['Effect_Mean'] > 0 else \"负向\" if row['Effect_Mean'] < 0 else \"中性\"\n",
    "                print(f\"  {i+1}. {row['Feature']}: 作用 {row['Effect_Mean']:.6f} ({direction}), 重要性 {row['Importance_Mean']:.6f}\")\n",
    "        \n",
    "        return {\n",
    "            'head_df': head_df,\n",
    "            'head_stats': head_stats,\n",
    "            'conc_df': conc_df,\n",
    "            'conc_stats': conc_stats,\n",
    "            'results_dir': results_dir\n",
    "        }\n",
    "    else:\n",
    "        print(\"❌ 没有成功分析任何模型\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_complete_effect_analysis(head_model, conc_model, val_loader, config):\n",
    "\n",
    "\n",
    "    print(\"🚀 开始完整的特征作用蜂群图分析\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "\n",
    "    for criterion in ['r2']:\n",
    "        print(f\"\\n📊 分析评估标准: {criterion}\")\n",
    "        \n",
    "        result = run_effect_swarm_analysis(\n",
    "            head_model, conc_model, val_loader, config,\n",
    "            evaluation_criterion=criterion, n_samples=300\n",
    "        )\n",
    "        \n",
    "        results[criterion] = result\n",
    "        \n",
    " \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\n✅ 完整的特征作用蜂群图分析完成!\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = run_complete_effect_analysis(head_model, conc_model, val_loader, config)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
